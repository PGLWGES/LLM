{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO2hsoMQnWRGHMSdpUlsLL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PGLWGES/LLM/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1 requeiment"
      ],
      "metadata": {
        "id": "mLumiF7EfgcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9R3uV_FMJg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26db5011-dd0c-4c64-95fa-c496d5022de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.8.4\n"
          ]
        }
      ],
      "source": [
        "!pip install uv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/requirements.txt --system"
      ],
      "metadata": {
        "id": "Fem36eulMQbW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f90a4a-ae43-485e-d2a3-f1b92be4fe89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m149 packages\u001b[0m \u001b[2min 919ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2masync-lru            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.93 KiB\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2masync-lru            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.93 KiB\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2masync-lru            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.93 KiB\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2masync-lru            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.93 KiB\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2masync-lru            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.93 KiB\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3986-validator    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/4.14 KiB\n",
            "\u001b[2masync-lru            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.93 KiB\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3986-validator    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/4.14 KiB\n",
            "\u001b[2masync-lru            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.93 KiB\n",
            "\u001b[2mrfc3987-syntax       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.86 KiB\n",
            "\u001b[2mtypes-python-dateutil\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.31 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3339-validator       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 3.41 KiB/3.41 KiB\n",
            "\u001b[2mrfc3986-validator       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 4.14 KiB/4.14 KiB\n",
            "\u001b[2mrfc3987-syntax          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.86 KiB/7.86 KiB\n",
            "\u001b[2mfqdn                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.91 KiB/8.91 KiB\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2moverrides               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.91 KiB/17.41 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 30.92 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.95 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.93 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.94 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/13.17 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3339-validator       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 3.41 KiB/3.41 KiB\n",
            "\u001b[2mrfc3987-syntax          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.86 KiB/7.86 KiB\n",
            "\u001b[2mfqdn                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.91 KiB/8.91 KiB\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2moverrides               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.41 KiB/17.41 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 30.92 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.95 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.94 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 397.42 KiB/20.09 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3339-validator       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 3.41 KiB/3.41 KiB\n",
            "\u001b[2mrfc3987-syntax          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.86 KiB/7.86 KiB\n",
            "\u001b[2mfqdn                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.91 KiB/8.91 KiB\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2moverrides               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.41 KiB/17.41 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 30.92 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.95 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.94 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 397.42 KiB/20.09 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mrfc3987-syntax          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.86 KiB/7.86 KiB\n",
            "\u001b[2mfqdn                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.91 KiB/8.91 KiB\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2moverrides               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.41 KiB/17.41 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 30.92 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.95 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.94 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 429.42 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/23.50 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mfqdn                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.91 KiB/8.91 KiB\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2moverrides               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.41 KiB/17.41 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 30.92 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 30.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.95 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 30.93 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.94 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 61.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 429.42 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 18.01 KiB/53.70 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mfqdn                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.91 KiB/8.91 KiB\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2moverrides               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.41 KiB/17.41 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 30.92 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 30.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.95 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 203.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 30.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 38.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 61.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 493.42 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 447.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 18.01 KiB/53.70 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2mfqdn                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 8.91 KiB/8.91 KiB\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 35.23 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 30.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 46.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 46.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 48.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 203.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 30.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 38.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 77.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 493.42 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 447.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 22.50 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 30.86 KiB/122.01 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 35.23 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 62.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.91 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 46.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 46.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 48.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 219.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 30.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 38.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 77.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 541.53 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 527.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 22.50 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 30.86 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 510.21 KiB/197.84 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjson5                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 35.23 KiB/35.23 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 62.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 62.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 48.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 219.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 46.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 38.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 93.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 733.53 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 703.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 51.51 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 589.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 670.21 KiB/197.84 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2muri-template            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.88 KiB/10.88 KiB\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 62.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 62.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 48.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 219.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 46.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 38.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 93.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 733.53 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 703.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 51.51 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 717.65 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 670.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 702.62 KiB/201.66 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-server-terminals\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.34 KiB/13.34 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.92 KiB/17.31 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 62.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 62.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 48.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 219.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 46.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 54.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 109.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 749.53 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 719.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 51.51 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 728.85 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 670.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 718.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 669.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[23A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mtypes-python-dateutil   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.31 KiB/17.31 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 62.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 62.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 48.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 219.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 46.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 54.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 109.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 818.61 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 799.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 63.14 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 797.65 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 750.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 798.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 749.52 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[22A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 14.94 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.90 KiB/58.30 KiB\n",
            "\u001b[2marrow                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 62.96 KiB/64.86 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 62.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 48.00 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 235.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 46.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 54.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 109.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 818.61 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 799.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 63.14 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 797.65 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 750.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 798.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 749.52 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[21A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mpython-json-logger      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.81 KiB/14.81 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 18.97 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 62.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 235.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 62.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 54.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 125.66 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 861.53 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 863.53 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.65 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 829.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 814.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 862.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 797.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[20A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 18.97 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 62.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 251.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 62.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 70.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 125.66 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.02 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.65 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 877.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 846.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 878.86 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 829.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[19A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 18.97 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 62.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 251.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 62.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 70.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 125.66 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.02 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 143.14 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 877.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 862.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 894.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 829.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[19A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 18.97 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 62.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 267.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 62.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 70.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 125.66 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.02 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 143.14 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 877.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 862.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 894.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 829.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[19A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2misoduration             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.06 KiB/11.06 KiB\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 18.97 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 62.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 287.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 62.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 70.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 125.66 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.02 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 895.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 428.23 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 877.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 862.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 894.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 829.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[19A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2mjupyter-events          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 18.97 KiB/18.97 KiB\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.84 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 62.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 287.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 62.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 70.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 125.66 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 927.02 KiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 926.45 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 444.23 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 893.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 878.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 910.62 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 861.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[18A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 73.10 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 62.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 303.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 62.91 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 70.68 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 125.66 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.01 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1007.64 KiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 444.23 KiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1021.76 KiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 974.21 KiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1006.52 KiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 957.74 KiB/346.60 MiB\n",
            "\u001b[2K\u001b[17A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 46.90 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 26.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 94.91 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 78.95 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 78.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 335.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 78.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 76.83 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 157.77 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.14 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.18 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.09 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.16 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.15 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.15 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.09 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[17A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 58.30 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 42.48 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-client          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 103.62 KiB/103.62 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 108.43 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 110.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 473.86 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 110.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 111.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 712.39 KiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.44 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.40 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.39 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.44 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.45 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.43 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.40 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[17A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2mjupyterlab-server       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 58.30 KiB/58.30 KiB\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 42.48 KiB/67.75 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 108.43 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 110.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 489.86 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 126.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 111.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.48 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.52 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.48 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.47 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.52 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.53 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[16A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 42.48 KiB/67.75 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 108.43 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 110.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 489.86 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 126.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 111.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.62 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.65 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.64 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.61 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.69 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.68 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.68 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.64 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[15A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/31)\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 58.48 KiB/67.75 KiB\n",
            "\u001b[2mlark                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 108.43 KiB/108.43 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 126.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 521.86 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 126.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 111.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.62 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.65 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.64 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.61 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.69 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.68 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.68 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.64 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[15A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 67.75 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 126.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 559.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 158.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 111.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.78 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.80 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.79 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.83 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[14A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjupyter-lsp             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 67.75 KiB/67.75 KiB\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 126.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 559.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 189.92 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 111.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.89 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.92 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.79 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.87 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.83 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.96 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.95 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.91 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[14A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 126.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 559.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 189.92 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 111.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.89 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.92 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.92 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.87 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.96 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.96 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.95 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.91 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[13A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 158.78 KiB/377.84 KiB\n",
            "\u001b[2mnvidia-cuda-runtime-cu12\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 607.38 KiB/863.02 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 237.25 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 127.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 2.03 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.07 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 2.19 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 2.16 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.24 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.11 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.06 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.04 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[13A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 174.78 KiB/377.84 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 237.25 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 127.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 2.18 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.20 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 2.20 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 2.18 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.24 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.24 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.21 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.18 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[12A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjupyter-server          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 366.78 KiB/377.84 KiB\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 253.25 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 127.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.32 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.34 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 2.33 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 2.18 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.38 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.36 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.35 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[12A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 269.15 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 127.23 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.32 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.34 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 2.33 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 2.30 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.38 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.36 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.35 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (15/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 269.15 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 140.83 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.54 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.50 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.56 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 2.45 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.62 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.55 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.52 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 269.15 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 172.73 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.89 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 2.90 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.92 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 2.90 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.97 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.94 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.94 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 2.89 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 172.73 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 3.33 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 3.34 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 3.34 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 3.34 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.39 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.37 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.36 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 188.73 KiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.65 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 3.59 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 3.56 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 3.56 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.72 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.58 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.70 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.68 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.01 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.89 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 3.97 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 3.99 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 4.06 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.99 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.96 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.95 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 3.93 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.18 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 4.38 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 4.39 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 4.42 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 4.25 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 4.40 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.43 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.38 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.23 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 4.79 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 4.81 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 4.81 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 4.83 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 4.75 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.86 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.26 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 5.17 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 5.31 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 5.26 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 5.25 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 5.23 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 5.36 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 5.18 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 5.22 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.32 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 5.53 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 5.72 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 5.67 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 5.70 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 5.64 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 5.76 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 5.64 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 5.68 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.40 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 6.07 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 6.11 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 6.18 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 6.20 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 6.04 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 6.31 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 6.18 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 6.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 6.49 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 6.70 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 6.65 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 6.62 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 6.56 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 6.72 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 6.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 6.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.49 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.89 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 7.24 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 7.14 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 7.06 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 7.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 7.34 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 7.15 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 7.05 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.65 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 7.47 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 7.64 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 7.53 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 7.59 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 7.50 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 7.70 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 7.55 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 7.61 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 286.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.68 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 7.91 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 8.15 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 8.01 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 8.07 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 7.95 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.20 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.05 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 8.09 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.84 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 8.43 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 8.70 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 8.50 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 8.59 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 8.47 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.73 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 8.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 8.64 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.85 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 9.06 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 9.17 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 9.00 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 9.07 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 8.92 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 9.22 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 9.03 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 9.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.87 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 9.37 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 9.75 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 9.52 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 9.56 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 9.46 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 9.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 9.41 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 9.60 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.95 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 9.89 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 10.14 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 9.99 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 9.93 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 9.89 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 10.14 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 9.94 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 10.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.98 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 10.35 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 10.62 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 10.36 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 10.47 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 10.35 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 10.56 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 10.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 10.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.07 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 10.75 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 11.03 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 10.77 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 10.89 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 10.73 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 10.97 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 10.80 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 10.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 10.95 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 11.21 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 10.97 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 11.07 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 11.11 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.15 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.19 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 11.28 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 302.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.17 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 11.34 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 11.59 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 11.34 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 11.45 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 11.32 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.50 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.58 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 11.75 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 12.00 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 11.72 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 11.86 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 11.69 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.79 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 11.89 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.49 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 12.10 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 12.39 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 12.12 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 12.24 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 12.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.37 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.60 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 12.39 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 12.70 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 12.34 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 12.55 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 12.39 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.56 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.69 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 12.79 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 13.13 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 12.80 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 12.94 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 12.83 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 12.89 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 13.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.63 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-cupti-cu12  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 13.17 MiB/13.17 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 13.53 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 13.33 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 13.37 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 13.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 13.36 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 13.31 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 13.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[11A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.65 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 13.98 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 13.64 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 13.81 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 13.70 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 13.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 13.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 13.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.65 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 14.12 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 13.75 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 13.95 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 13.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 13.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 13.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (20/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 14.75 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.20 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 14.37 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.48 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.30 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 318.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.83 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 15.06 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.65 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 14.84 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.79 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.84 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.86 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 334.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.88 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 15.50 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 15.26 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 15.31 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 15.43 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 15.50 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 15.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 15.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 334.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.93 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 16.20 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 15.78 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 16.00 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 16.01 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 15.97 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 15.83 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.05 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 334.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 16.73 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 16.36 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 16.44 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 16.51 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.48 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.36 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 334.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.42 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 17.09 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.78 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 16.95 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 17.05 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.80 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 17.01 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 350.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.46 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 17.64 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 17.24 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 17.49 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 17.55 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 17.49 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 17.24 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 17.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 350.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.53 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 18.09 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 17.92 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 18.09 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 18.03 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 17.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 17.73 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 18.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 350.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.53 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 18.76 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 18.42 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 18.54 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 18.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 18.63 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 18.20 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 18.43 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 350.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.60 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 19.22 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 18.71 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 18.97 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 19.12 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 18.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 18.89 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 19.09 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 350.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.81 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 19.53 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 19.33 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 19.45 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 19.59 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 19.48 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 19.31 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 19.59 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 367.40 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.89 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-nvjitlink-cu12   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 20.09 MiB/20.09 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 19.89 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 20.01 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 20.04 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 19.99 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 19.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 20.18 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 367.40 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.89 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 20.06 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 20.18 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 20.22 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 20.13 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 20.01 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 20.32 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (21/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 367.40 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 3.91 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 20.59 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 20.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 20.55 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 20.53 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 20.28 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 20.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 367.40 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 3.95 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.16 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 21.10 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 21.06 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.07 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 20.78 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 21.24 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 383.40 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 4.12 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.60 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 21.41 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 21.62 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.56 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.28 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 21.67 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 419.46 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 4.53 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.84 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 21.41 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.05 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.21 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.34 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 21.67 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 510.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.84 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 21.41 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.13 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.29 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.34 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 21.72 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 558.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.84 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 21.41 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.37 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.51 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 558.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.84 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.66 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.78 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 638.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.84 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.75 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 686.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.84 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.77 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.87 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 750.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 21.84 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.77 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.89 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 830.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.85 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.96 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 862.68 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.87 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.96 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 942.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.90 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 958.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.72 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.95 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.95 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.04 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 990.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.79 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.95 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 22.97 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 1004.50 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.09 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 22.95 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.04 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.09 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 1022.03 KiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.09 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.22 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.29 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 21.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 1.03 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.09 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.38 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.43 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.45 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 1.03 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.09 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.38 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.43 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.45 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 1.06 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.09 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.38 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.43 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.45 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 1.06 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.21 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.05 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.46 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.54 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 1.09 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.21 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.52 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.61 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.10 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.54 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.62 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.12 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.55 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.13 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.45 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 23.23 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.62 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.68 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.13 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.45 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.38 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.62 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.70 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 1.15 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 5.53 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.69 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.75 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 22.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 1.16 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 23.84 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 23.07 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 1.18 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 24.03 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.12 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 23.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 1.21 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.23 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 24.03 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.14 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 23.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 1.21 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.24 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 24.03 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.14 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 23.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 1.21 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 24.07 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.19 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 22.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 23.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 1.23 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 24.20 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.34 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.45 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjedi                    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 1.26 MiB/1.50 MiB\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.45 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (22/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.45 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.46 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.50 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.67 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.51 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.50 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.67 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 23.51 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.50 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.67 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.15 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 24.10 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.67 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.86 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.18 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 24.10 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.88 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.18 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 24.10 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.88 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.49 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.18 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.45 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 24.10 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.88 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.18 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 22.94 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 24.10 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.88 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 23.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.25 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cuda-nvrtc-cu12  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 23.50 MiB/23.50 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 24.84 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.84 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 25.18 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.70 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 24.93 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.25 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 24.84 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 24.84 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 25.18 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 24.70 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 25.17 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.25 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 25.57 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 25.61 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 25.87 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 25.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 25.83 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (23/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 26.04 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 26.23 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 26.36 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 26.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 26.17 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 26.76 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 26.89 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 26.72 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 26.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 26.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 27.18 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 27.56 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 27.42 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 27.18 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 27.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 27.62 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 28.31 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 28.37 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 27.62 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 28.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 28.61 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 28.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 29.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 28.58 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 29.19 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 29.12 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 30.12 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 29.69 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 29.19 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 29.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 29.73 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 30.33 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 30.12 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 29.62 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 30.22 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 30.14 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 31.15 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 31.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 30.31 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 30.70 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 30.87 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 32.19 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 31.76 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 30.98 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 31.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 31.61 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 32.91 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 32.58 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 31.69 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.23 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 32.25 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 33.61 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 33.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 32.43 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 33.00 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 34.45 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 33.94 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 32.84 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 33.37 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.27 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 33.37 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 35.23 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 34.38 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 33.59 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 34.33 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.30 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 33.87 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 35.79 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 35.07 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 33.95 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 34.83 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.35 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 34.07 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 35.98 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 35.07 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 34.30 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 35.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.37 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 34.56 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 36.50 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 35.57 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 34.61 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 35.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 35.01 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 37.28 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 36.29 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 35.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 36.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 35.75 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 37.89 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 36.93 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 35.76 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 36.96 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 36.50 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 38.75 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 37.50 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 36.40 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 37.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 37.15 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 39.49 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 38.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 37.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 38.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 37.81 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 40.04 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 38.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 37.69 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 39.12 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 38.47 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 41.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 39.91 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 38.43 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 39.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 38.92 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 41.66 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 40.34 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 39.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 40.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 39.63 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 42.36 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 41.09 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 40.05 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 41.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 40.24 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 42.99 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 41.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 40.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 41.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.40 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 41.17 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 43.97 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 42.41 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 41.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 42.65 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.40 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 41.93 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 44.81 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 43.01 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 42.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 43.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.40 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 42.54 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 45.57 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 44.20 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 42.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 44.18 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.40 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 43.25 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 46.65 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 44.87 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 43.74 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 44.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 44.22 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 47.20 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 45.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 44.62 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 45.87 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 44.90 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 47.90 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 46.46 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 45.09 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 46.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 45.62 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 48.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 47.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 45.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 46.98 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.62 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 49.45 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 47.99 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 46.47 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 47.90 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 47.51 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 50.53 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 48.54 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 47.06 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.34 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 48.25 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 51.07 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 49.54 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 47.94 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 49.18 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 48.95 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 52.00 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 50.23 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 48.43 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 50.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.41 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 49.55 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 52.56 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 50.61 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 49.14 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 50.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 50.19 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 53.39 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 51.44 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 49.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 51.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 50.81 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 54.52 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 52.22 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 50.47 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 51.79 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 51.53 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 55.19 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 52.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 51.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 52.47 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 52.06 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 55.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 53.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 51.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 53.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 52.34 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 56.17 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 53.56 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 52.08 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 53.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 53.19 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 56.44 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 54.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 52.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 53.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 53.41 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 57.24 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 54.62 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 53.18 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 54.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-curand-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 53.70 MiB/53.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 57.95 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 55.48 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 53.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 55.23 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 58.64 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 55.72 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 54.27 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 56.26 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 59.21 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 56.09 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 54.45 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 56.40 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.43 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 60.07 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 56.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 55.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 57.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.46 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 60.82 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 57.75 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 55.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 57.67 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (24/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.51 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 61.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 58.50 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 56.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 58.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.54 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 62.39 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 58.99 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 57.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 59.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.61 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 63.31 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 59.91 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 58.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 59.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.66 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 63.84 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 60.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 58.79 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 60.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.71 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 64.55 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 61.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 59.58 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 61.20 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.78 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 65.11 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 61.97 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 60.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 61.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.88 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 65.93 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 62.76 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 60.99 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 62.85 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.88 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 66.73 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 63.45 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 61.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 63.57 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 7.07 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 67.26 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 64.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 62.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.29 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.61 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 67.92 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 64.61 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 62.77 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.61 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 69.11 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 65.44 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 63.54 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 65.20 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.77 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 69.95 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 66.12 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 64.28 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 65.71 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.79 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 70.53 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 66.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 65.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 66.70 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.80 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 71.36 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 67.33 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 65.67 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 67.16 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.88 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.31 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 67.91 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 66.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 68.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.88 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.49 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.72 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 67.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 68.65 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.91 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.37 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 67.95 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 8.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.88 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.11 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 8.12 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.03 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.13 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.13 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.34 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 72.87 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.34 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.41 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.07 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.46 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.23 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.54 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 68.97 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.23 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.08 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.23 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.08 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.25 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.08 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.26 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.08 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 69.28 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.15 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 73.13 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.15 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.27 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.15 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.68 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.78 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.30 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.69 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.31 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.76 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.34 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.83 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.83 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.36 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.88 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.38 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.90 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.38 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.91 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.38 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.92 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.38 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.97 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.38 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.04 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.34 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.43 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.41 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.04 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.40 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.43 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.04 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.05 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.06 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.05 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.07 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.05 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.10 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.54 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.17 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.08 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.18 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.08 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.23 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.09 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.28 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.09 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.57 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.28 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.11 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.59 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.28 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.11 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.59 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.29 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.11 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.59 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 9.40 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.14 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.63 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.52 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 9.40 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.14 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.63 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 9.45 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.63 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.20 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 9.62 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.38 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.84 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 9.67 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.41 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.86 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 9.71 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.42 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.88 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 9.76 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.42 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.57 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.98 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.66 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.57 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.98 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.67 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.57 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.98 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.69 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.57 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.98 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.69 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.57 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.98 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.70 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.88 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.57 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 69.98 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.00 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 73.95 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.61 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.02 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.02 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 74.16 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.69 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.11 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.03 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 74.16 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.69 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.11 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.08 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 74.16 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.71 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.11 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 69.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.08 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 74.77 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.71 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.13 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 70.29 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 10.51 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 75.89 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.71 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.13 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 70.96 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 10.90 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 76.15 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 72.05 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 72.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.50 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 76.31 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 72.37 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 70.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 72.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.55 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 76.90 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 72.37 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.09 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 72.42 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.56 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 77.26 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 72.94 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.42 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 72.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.56 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 77.73 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 73.73 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 71.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 73.12 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.56 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 78.65 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 74.31 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 72.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 73.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.56 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 79.43 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 74.67 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 72.94 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 73.89 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.61 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 80.00 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 74.97 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 73.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 74.01 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mjupyterlab              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.70 MiB/11.70 MiB\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 80.23 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.50 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 73.96 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 74.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 80.23 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.50 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 73.96 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 75.54 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 80.50 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.76 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 74.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 75.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (25/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 80.99 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 76.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 74.94 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 76.15 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 80.99 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 76.35 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 76.74 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 81.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 76.76 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 77.05 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 81.68 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.12 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.48 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 77.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 81.95 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.42 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 77.64 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 82.39 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.83 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 75.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 77.64 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 82.39 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.83 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 76.18 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 77.98 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 83.26 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 78.11 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 76.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 77.98 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 83.41 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 78.82 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 76.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 78.73 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 83.87 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 79.22 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.21 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 79.24 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 84.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 79.63 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.44 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 79.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 84.50 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 80.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 80.51 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 84.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 80.62 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 78.44 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 81.41 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 85.37 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 81.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 79.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 81.77 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 85.37 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 81.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 79.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 81.86 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 85.86 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 81.53 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 79.45 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 82.69 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 86.81 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 81.89 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 80.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 82.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 87.19 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 82.41 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 80.28 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 83.87 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 87.58 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 83.12 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 80.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 83.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 88.71 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 83.54 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 81.15 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 84.65 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 89.15 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 83.98 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 81.68 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 85.34 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 90.02 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 84.18 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 81.97 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 85.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 90.66 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 84.62 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 82.49 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 86.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 91.64 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 85.03 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 83.05 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 86.34 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 91.64 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 85.42 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 83.39 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 86.65 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 92.81 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 85.78 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 83.61 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 87.22 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 93.32 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 86.19 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 84.06 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 88.12 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 94.05 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 86.86 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 84.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 88.47 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 94.40 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 87.26 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 85.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 89.27 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 95.18 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 87.93 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 85.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 89.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 95.76 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 88.45 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 86.24 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 90.52 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 96.12 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 89.16 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 86.95 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 90.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 97.54 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 89.31 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 87.18 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 91.74 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 98.45 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 90.20 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 88.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 92.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 98.97 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 90.47 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 88.61 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 92.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 99.84 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 91.58 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 89.10 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 93.37 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 100.52 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 92.44 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 89.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 94.12 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 102.20 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 92.94 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 90.61 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 94.98 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 103.06 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 93.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 91.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 95.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 104.07 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 94.50 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 91.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 96.30 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 105.51 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 95.18 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 92.69 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 96.91 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 106.55 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 95.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 93.20 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 97.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 106.92 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 96.52 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 93.84 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 98.43 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 107.93 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 97.37 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 94.40 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 99.12 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 108.75 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 98.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 95.61 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 100.37 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 109.09 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 98.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 96.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 101.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 109.87 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 99.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 96.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 101.91 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 110.44 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 99.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 96.69 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 102.89 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 110.70 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 99.87 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 97.25 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 103.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 111.25 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 100.40 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 98.02 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 104.25 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 111.81 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 101.21 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 98.25 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 105.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 112.81 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 101.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 98.79 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 106.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 113.05 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 101.93 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 99.15 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 106.51 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 113.28 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 102.44 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 99.67 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 107.18 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 113.94 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 102.98 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 100.30 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 107.96 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 114.54 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 103.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 100.84 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 108.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 115.15 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 103.78 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 101.51 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 108.96 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 115.49 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 104.43 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 101.73 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 109.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 116.05 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 104.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 102.03 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 109.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 116.47 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 104.99 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 102.64 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 110.19 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 117.20 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 105.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 103.02 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 111.05 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 117.90 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 106.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 103.78 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 111.87 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 118.48 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 107.24 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 104.48 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 112.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 119.73 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 108.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 105.25 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 113.25 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 120.89 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 108.65 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 105.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 113.86 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 121.45 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 109.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 107.05 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 114.79 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 122.00 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 110.58 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 107.36 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 115.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 122.01 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 111.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 108.24 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 116.87 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 122.01 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 112.65 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 109.26 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 117.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusolver-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 122.01 MiB/122.01 MiB\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 114.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 111.02 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 118.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 114.61 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 111.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 119.18 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 115.32 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 112.03 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 119.61 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 116.56 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 112.79 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 121.49 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (26/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 116.87 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 113.44 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 121.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 117.53 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 114.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 123.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 118.18 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 114.79 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 123.36 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 119.31 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 115.22 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 124.37 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 120.05 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 116.09 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 124.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 120.34 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 116.69 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 125.42 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 121.93 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 117.84 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 126.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 123.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 118.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 127.43 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 123.83 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 120.27 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 128.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 125.22 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 121.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 129.83 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 125.84 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 122.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 130.09 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 126.86 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 123.86 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 130.92 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 128.69 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 124.51 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 131.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 129.67 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 125.44 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 132.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 130.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 126.34 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 133.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 131.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 127.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 134.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 132.20 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 128.10 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 135.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 132.97 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 129.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 136.42 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 134.28 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 129.64 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 137.19 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 134.85 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 130.06 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 137.61 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 135.61 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 130.92 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 139.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 136.72 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 131.69 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 140.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 137.81 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 133.03 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 141.37 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 138.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 133.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 143.16 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 139.98 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 134.95 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 143.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 140.80 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 136.19 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 145.20 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 141.89 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 136.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 145.86 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 143.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 137.09 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 146.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 143.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 137.80 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 147.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 144.12 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 138.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 149.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 145.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 139.58 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 149.45 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 146.78 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 140.60 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 151.05 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 148.32 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 141.48 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 152.63 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 149.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 142.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 153.45 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 150.31 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 143.19 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 154.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 150.68 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 144.07 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 156.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 152.40 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 144.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 156.74 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 153.35 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 146.22 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 158.34 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 154.69 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 146.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 159.70 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 155.98 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 148.03 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 160.92 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 156.53 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 148.33 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 162.01 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 157.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 149.21 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 162.59 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 157.87 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 150.76 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 163.69 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 158.86 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 152.14 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 165.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 159.37 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 152.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 166.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 160.40 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 153.48 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 167.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 161.21 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 154.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 168.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 154.78 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 169.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 163.58 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 155.44 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 170.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 164.09 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 156.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 171.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.68 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 157.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 173.43 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 157.70 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 158.55 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 158.70 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 158.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 159.54 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.92 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 159.89 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.14 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 165.93 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 160.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.72 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 166.42 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 160.38 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.72 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 161.03 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.17 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 161.05 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.22 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.20 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 161.14 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 161.39 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 161.68 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 161.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 161.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 161.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.10 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.55 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 167.90 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.22 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.01 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.22 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.02 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.22 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.04 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.22 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.06 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.22 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.48 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.92 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.48 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 175.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.10 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.10 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.10 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.10 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.79 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 162.87 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.15 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 163.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 164.55 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 164.55 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.48 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.49 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.52 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.90 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.55 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.52 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.92 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.62 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.52 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.52 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.52 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.52 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.55 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.58 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.59 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.61 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 165.62 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.16 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.16 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.33 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.08 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.39 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.33 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.16 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.42 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 170.64 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.46 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.51 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.54 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 166.90 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.35 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.43 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.17 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 177.79 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.23 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.35 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.72 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.35 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.55 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.59 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 178.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 171.95 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 167.78 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 180.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 174.08 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 168.78 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 180.92 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 175.03 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 169.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 182.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 175.62 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.03 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 184.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 177.61 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 171.53 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 184.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 179.14 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 173.02 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 186.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 180.22 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 173.58 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 187.52 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 180.73 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 174.50 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 188.23 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 182.21 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 175.58 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 189.33 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 183.78 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 176.81 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 190.51 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 185.00 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 177.78 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 192.31 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 185.93 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 179.23 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 194.26 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 187.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 180.65 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 195.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 188.83 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 181.56 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 195.58 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 190.18 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 182.75 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 196.37 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 190.59 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 183.11 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 198.42 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 191.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 184.34 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 199.13 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 192.25 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 185.79 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 199.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 193.09 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 186.92 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 200.26 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 193.51 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 187.12 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 201.98 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 194.12 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 188.00 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 203.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 194.98 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 189.08 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 203.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 195.98 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 189.84 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 204.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 197.15 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 190.82 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 205.98 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 197.82 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 191.91 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 206.86 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 197.84 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 193.76 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 208.69 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cusparse-cu12    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 197.84 MiB/197.84 MiB\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 195.08 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 210.56 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 195.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 210.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (27/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 195.37 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 210.97 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 196.94 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 212.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 198.05 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 214.40 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 198.79 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 215.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 200.72 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 216.65 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 201.32 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 219.02 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 201.65 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 220.69 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cufft-cu12       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 201.66 MiB/201.66 MiB\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 222.51 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 222.51 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (28/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 225.17 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 227.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 228.51 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 231.15 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 233.09 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 235.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 235.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 236.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 238.50 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 240.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 240.72 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 241.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 243.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 243.84 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 245.45 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 246.55 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 247.14 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 248.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 249.45 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 251.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 252.26 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 252.68 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 253.58 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 254.46 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 256.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 256.43 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 258.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 259.70 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 261.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 262.20 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 263.48 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 265.12 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 267.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 268.71 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 269.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 270.20 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 271.12 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 272.98 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 274.80 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 276.26 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 277.73 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 279.29 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 280.59 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 282.22 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 283.95 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 285.71 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 287.06 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 287.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 289.94 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 291.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 292.51 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 293.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 294.96 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 295.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 295.78 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 296.16 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 296.49 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 297.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 298.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 299.76 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 301.00 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 302.42 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 302.75 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 303.22 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 304.45 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 306.25 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 308.03 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 310.53 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 312.81 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 314.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 315.68 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 317.47 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 319.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 321.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 322.70 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 325.11 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 325.26 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 327.83 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 328.39 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 329.15 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 331.44 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 334.17 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 337.25 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 339.59 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 342.62 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 346.52 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2mnvidia-cublas-cu12      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 346.60 MiB/346.60 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (29/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (30/31)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m31 packages\u001b[0m \u001b[2min 30.42s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m12 packages\u001b[0m \u001b[2min 43ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m31 packages\u001b[0m \u001b[2min 152ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1marrow\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masync-lru\u001b[0m\u001b[2m==2.0.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfqdn\u001b[0m\u001b[2m==1.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1misoduration\u001b[0m\u001b[2m==20.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjson5\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==6.1.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-events\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-lsp\u001b[0m\u001b[2m==2.2.6\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjupyter-server\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-server\u001b[0m\u001b[2m==2.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-server-terminals\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyterlab\u001b[0m\u001b[2m==4.4.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyterlab-server\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moverrides\u001b[0m\u001b[2m==7.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-json-logger\u001b[0m\u001b[2m==3.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrfc3339-validator\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrfc3986-validator\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrfc3987-syntax\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtypes-python-dateutil\u001b[0m\u001b[2m==2.9.0.20250708\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muri-template\u001b[0m\u001b[2m==1.3.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Working with text data"
      ],
      "metadata": {
        "id": "zagxD3bn2Oce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Understanding word embeddings"
      ],
      "metadata": {
        "id": "48nKdPlGfyPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Tokenizing text"
      ],
      "metadata": {
        "id": "HJhjN3-ef37O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# urllib.request 打开URL（主要是HTTP）\n",
        "import urllib.request\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "     \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "     \"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "# 从指定的URL下载数据\n",
        "urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "pxk5lipH2zEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5078488f-394f-4edc-ced0-43ce2553af14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x7d91df451e50>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "id": "7M3RYKY93BUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a52336e-37bb-4281-b960-179e84403d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入正则表达式模块\n",
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qSV70NE13TKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41502dc-2a9f-4a37-fd73-09a5ec91b24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "y6WBOylP3cXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3127a0-ccf7-4f17-ae19-c66050fc48aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 移除空字符串\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4s_cUcrA3436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614bc132-1fea-4f48-8830-56eec5a7c3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "# 对每个item调用strip() 移除首尾两端的空白字符\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "fu7X1dJf4i4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368db13e-9bec-4b93-f3af-ba9acca5a2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "id": "zz70QNZ94pWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ba0276-3299-44c4-dee1-4edba03b74e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "id": "Fx9bfaFK4xTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0e7d15-3361-4a95-8ab1-247358efd903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Converting tokens into token IDs"
      ],
      "metadata": {
        "id": "Dk9SY6wDf9Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "-0qT-1orWgCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c05ec14-aadf-403a-b2bd-c12b71bccabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab = {}\n",
        "# for integer, token in enumerate(all_words):\n",
        "#     vocab[token] = integer  # 键为token，值为integer\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 50:\n",
        "    break"
      ],
      "metadata": {
        "id": "6L-Jx5Oc4zTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d94186-84a7-4930-ccb5-850facb3de92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个简单的文本分词器类，用于编码（文本转ID）和解码（ID转文本）\n",
        "class SimpleTokenizerV1:\n",
        "    # 初始化方法，接收词汇表（vocab）作为参数\n",
        "    def __init__(self, vocab):\n",
        "        # 存储 字符串（token）到整数（ID）的映射\n",
        "        # 字典结构 {键：值} self.str_to_int[key] = value\n",
        "        self.str_to_int = vocab\n",
        "        # 构建 整数（ID）到字符串（token）的反向映射，用于解码\n",
        "        # 反向映射 {值：键}\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    # 编码方法：将输入文本转换为 token ID 列表\n",
        "    def encode(self, text):\n",
        "        # 步骤1：用正则表达式分割文本，拆分出 token（包含标点、单词等）\n",
        "        # 正则匹配逻辑\n",
        "        preprocessed = re.split(r'([,.?!\\'\"\\[\\]]|--|\\s)', text)\n",
        "        # 步骤2：过滤空内容（strip后为空的项），并去除每个 token 首尾空白\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        # 步骤3：通过词汇表映射，将每个字符串 token 转为对应的整数 ID\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids  # 返回编码后的 ID 列表\n",
        "\n",
        "    # 解码方法：将 token ID 列表转换为原始文本\n",
        "    def decode(self, ids):\n",
        "        # 步骤1：通过反向映射，将 ID 转回字符串 token，并用空字符串拼接\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # 步骤2：用正则表达式清理标点前的多余空格\n",
        "        # 匹配逻辑：标点（,.?!'\"\\[]）前的一个或多个空格，替换为单个空格（保证标点前无冗余空格）\n",
        "        text = re.sub(r'\\s+([,.?!\\'\"\\[\\]])', r'\\1', text)\n",
        "        return text  # 返回解码后的文本"
      ],
      "metadata": {
        "id": "uHsVcibBWWJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "oWlilDPpYS7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4354c62c-3c7c-4f54-9de0-886185483b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "MVo8pGpDYYlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea34c65-aecb-4e51-8c5c-edc1d5e2528f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Hello, do you like tea?\"\n",
        "# print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "40xWKn2ObAQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Adding special context tokens"
      ],
      "metadata": {
        "id": "2clX8sdZgDel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "id": "1ewOYWPPbHlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf551b1-12c6-4e67-ba88-0574df810a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "tQa_CCiLbQsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da9fe6d0-2a8a-43ac-ac6f-f558da75b75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个增强版的简单文本分词器类（相比V1版本增加了OOV处理）\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        # 存储 字符串→ID 的映射（来自传入的词汇表）\n",
        "        self.str_to_int = vocab\n",
        "        # 构建 反向映射（ID→字符串），用于解码\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        # 步骤1：使用正则表达式将文本拆分为tokens（包括标点和空格）\n",
        "        preprocessed = re.split(r'([,.?!\\'\"\\[\\]]|--|\\s)', text)\n",
        "\n",
        "        # 步骤2：过滤空字符串，并去除每个token的首尾空格\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "        # 步骤3：处理未登录词（OOV）：将词汇表中不存在的token替换为<|unk|>\n",
        "        # 关键增强点：相比V1版本，增加了对未知词的处理，避免KeyError\n",
        "        preprocessed = [\n",
        "            # 如果token在词汇表中，保留原样\n",
        "            item if item in self.str_to_int\n",
        "            # 否则替换为特殊的未知词标记\n",
        "            else \"<|unk|>\"\n",
        "            for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        # 步骤4：将tokens转换为对应的ID\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # 步骤1：将ID列表转换回字符串tokens，并使用空格拼接（可能产生多余空格）\n",
        "        # 注意：此处使用空格拼接是为了后续正则处理的便利性\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        # 步骤2：修复标点符号前的多余空格（与V1版本相同）\n",
        "        # 例如：将 \"Hello , world\" 转换为 \"Hello, world\"\n",
        "        text = re.sub(r'\\s+([,.?!\\'\"\\[\\]])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "zdog5zHzbWcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "N2QvNdIkbaRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39ed102-d9f6-4c42-cf2c-1929c6be4f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "q44t-4EAbbzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9885d373-4710-405a-eec3-92e75a894c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "id": "PYTvRnJJbdvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73932078-151d-449e-9603-6de308b02dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Btye pair encoding"
      ],
      "metadata": {
        "id": "OotNO8zBgI3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "id": "LYFE8kFHbgDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d5abe9-1a71-4102-86c3-417c52eba5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importlib.metadata 访问已安装 Python 包的元数据\n",
        "from importlib.metadata import version\n",
        "# 计算 token 数量的库\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "id": "hl-AAJv3blL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc79b44-cd21-418d-f4f7-eb9fbf5e7a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用gpt-2的分词逻辑\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "PlW0jqekbpWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "id": "j4CXF5SUbrOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f45c67-907e-4fa7-8f3b-f59edfeae54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "id": "ZDvU-8ABbt9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c595f48-47df-445c-e60e-32b5f3717092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ip = \"Akwirw ier\"\n",
        "temp = tokenizer.encode(ip)\n",
        "print(temp)\n",
        "op = tokenizer.decode(temp)\n",
        "print(op)"
      ],
      "metadata": {
        "id": "NR45cjvXbw3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1afa0d-882e-47ca-8109-baf6c4ffce20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Data sampling with a silding window"
      ],
      "metadata": {
        "id": "HomBlCMcgNRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "id": "wsEfr3ivbzQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52520209-0145-44ac-a99b-3578f8625842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "98sHRQAhb4q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义上下文窗口大小，即每次取多少个连续的 token 作为输入上下文\n",
        "context_size = 4\n",
        "# 从编码后的样本中，取从开头到 context_size 位置的子序列作为输入特征 x\n",
        "# 比如 context_size=4 时，取 [0,1,2,3] 位置的 token\n",
        "x = enc_sample[:context_size]\n",
        "# 从编码后的样本中，取从第 1 个位置到 context_size+1 位置的子序列作为目标值 y\n",
        "# 相当于在 x 的基础上向后滑动一位，用于训练时让模型学习“根据 x 预测下一个 token（即 y 对应的序列）”\n",
        "y = enc_sample[1:context_size+1]\n",
        "# 打印输入特征 x 的内容，方便调试或查看数据\n",
        "print(f\"x: {x}\")\n",
        "# 打印目标值 y 的内容，观察模型需要预测的结果形式\n",
        "print(f\"y:    {y}\")"
      ],
      "metadata": {
        "id": "qjiVzcogb6NA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be414953-f9f7-4e46-f854-ea49b4514343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:    [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 遍历从 1 到 context_size（包含 context_size）的整数，实现滑动窗口的效果\n",
        "for i in range(1, context_size + 1):\n",
        "    # 截取从序列起始到索引 i 的子序列，作为模型的输入上下文\n",
        "    # 随着 i 增大，上下文长度从 1 逐渐增加到 context_size\n",
        "    context = enc_sample[:i]\n",
        "    # 选取索引 i 位置的元素，作为模型需要预测的目标\n",
        "    desired = enc_sample[i]\n",
        "    # 打印当前的输入上下文和对应的预测目标，方便观察滑动窗口过程\n",
        "    print(context, \"----->\", desired)"
      ],
      "metadata": {
        "id": "w5io2t2pcANs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6e9132-1176-466e-a615-37f8df3a0028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] -----> 4920\n",
            "[290, 4920] -----> 2241\n",
            "[290, 4920, 2241] -----> 287\n",
            "[290, 4920, 2241, 287] -----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "id": "kmcto7GmcCDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f8b21f-287d-4e2b-a1b2-696923e65bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "8-WwMymBcESx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b43144-dc8e-4888-bdca-3ab4a2c66e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset：一个抽象基类，用于定义自定义数据集\n",
        "# DataLoader：用于将 Dataset 包装成一个可迭代的数据加载器，方便在训练时高效地获取数据批次\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 定义一个名为 GPTDatasetV1 的数据集类，继承自 PyTorch 的 Dataset 类\n",
        "class GPTDatasetV1(Dataset):\n",
        "    # 初始化方法，用于创建数据集对象时设置参数和初始化数据\n",
        "    # txt: 原始文本数据\n",
        "    # tokenizer: 分词器，用于将文本转换为 token IDs\n",
        "    # max_length: 每个输入块（input chunk）的最大长度\n",
        "    # stride: 滑动窗口的步长，控制每次窗口移动的距离\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        # 初始化存储输入 token IDs 的列表\n",
        "        self.input_ids = []\n",
        "        # 初始化存储目标 token IDs 的列表\n",
        "        self.target_ids = []\n",
        "\n",
        "        # 使用分词器对整个文本进行编码，得到 token IDs 序列\n",
        "        # tokenizer.encode 方法会同时完成分词和转换为 token IDs 的操作\n",
        "        token_ids = tokenizer.encode(txt)  # Tokenizes the entire text（对整个文本进行分词）\n",
        "\n",
        "        # 滑动窗口遍历 token_ids 序列，生成输入块和目标块\n",
        "        # 遍历的起始位置从 0 开始，结束位置为 len(token_ids) - max_length（保证窗口不越界），步长为 stride\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            # 截取从 i 开始，长度为 max_length 的子序列作为输入块\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            # 截取从 i + 1 开始，长度为 max_length 的子序列作为目标块（输入块右移一位，用于预测下一个词）\n",
        "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
        "\n",
        "            # 将输入块转换为 PyTorch 张量，并添加到 input_ids 列表中\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            # 将目标块转换为 PyTorch 张量，并添加到 target_ids 列表中\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    # 用于返回数据集的样本数量，PyTorch 的 Dataset 类需要实现的方法\n",
        "    def __len__(self):\n",
        "        # 返回输入数据的数量，即 input_ids 列表的长度\n",
        "        return len(self.input_ids)  # Returns the total number of rows in the dataset（返回数据集中行的总数）\n",
        "\n",
        "    # 用于根据索引 idx 获取数据集中的一个样本（输入和对应的目标），PyTorch 的 Dataset 类需要实现的方法\n",
        "    def __getitem__(self, idx):\n",
        "        # 返回指定索引 idx 对应的输入张量和目标张量\n",
        "        return self.input_ids[idx], self.target_ids[idx]  # Returns a single row from the dataset（从数据集中返回一行数据）"
      ],
      "metadata": {
        "id": "rtl6LZwHcI5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "def create_dataloader_v1(txt,\n",
        "             batch_size=4,\n",
        "             max_length=256,\n",
        "             stride=128,\n",
        "             shuffle=True,\n",
        "             drop_last=True,\n",
        "             num_workers=0):\n",
        "    # 初始化分词器，使用 tiktoken 库的 get_encoding 方法获取 \"gpt2\" 对应的编码方式\n",
        "    # 分词器用于将文本转换为模型可处理的 token\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # 创建数据集实例，传入原始文本、分词器、最大长度、滑动步长等参数\n",
        "    # 该数据集类（GPTDatasetV1）需提前定义好，用于处理文本数据并生成输入 - 目标对\n",
        "    # 创建数据集\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # 使用 PyTorch 的 DataLoader 包装数据集，方便模型训练时按批次加载数据\n",
        "    dataloader = DataLoader(\n",
        "        dataset,  # 传入数据集对象\n",
        "        batch_size=batch_size,  # 每个批次的样本数量\n",
        "        shuffle=shuffle,  # 是否打乱数据顺序，True 表示训练时打乱增加随机性\n",
        "        drop_last=drop_last,\n",
        "        # 若最后一个批次样本数量不足 batch_size，是否丢弃该批次，避免训练时因批次不规整导致损失异常波动\n",
        "        num_workers=num_workers\n",
        "        # 用于数据预处理的 CPU 进程数，0 表示在主进程中进行预处理\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "dJiATZLocNeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "# 迭代器\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "4RoXQ0lGcSGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba25f3b-9a30-4ac4-f10a-999e1b2b1f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "id": "9HAYEuHKcWMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0de905-333a-4aff-bc93-587b8ddefe72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "id": "EjAJ5v9ScgZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8cedd3d-a542-417a-ba1e-adf7f6cecac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Creating token embeddings"
      ],
      "metadata": {
        "id": "1N9MY5X0gVWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 词汇表的索引\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "UxKST0_ActGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3"
      ],
      "metadata": {
        "id": "rtUaNlw6dA4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "id": "1aljD7ojczAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a30f76-032c-4359-be74-3f7ff3744f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "id": "wFhyZylFc7Nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4c2fea-d782-4b97-8c91-9cfad0a8fbcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "id": "eu0E7zWcdGXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2094db1-6b14-4608-f648-88235fa00841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Encoding word positions"
      ],
      "metadata": {
        "id": "2A0CXlDcgrXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "# 8是 batch_size（每个批次的样本数 ）\n",
        "# 4是 max_length（每个样本包含的 token 数量 ）\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "id": "dnJ2D3GEdMCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5b43b6-f7b7-40e7-ed75-6ebcf62239c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "CK-KU_SDeewa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188a2afc-abbe-47ad-9838-ade1ba3d68c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "# 创建一个 PyTorch 嵌入层，专门用于生成位置嵌入\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "# 生成从 0 到 context_length-1 的位置索引序列 嵌入\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "RoaIXTjYejlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa3b0d3-e25f-4c33-905c-a4c8ac56cae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "id": "TtaSsKaGenb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1fd326-abea-4e94-d41e-14418185499a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Coding attention mechanisms"
      ],
      "metadata": {
        "id": "UzIQFEwvfNUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 The problem with modeling long sequences"
      ],
      "metadata": {
        "id": "3-le9mQRg5zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Capturing data dependencies with attention mechanisms"
      ],
      "metadata": {
        "id": "aaEiQctGIVeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  3.3 Attending to different parts of the input with self-attention"
      ],
      "metadata": {
        "id": "9gAc2g5GIe8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.3.1 A simple self-attention mechanism without trainable weights"
      ],
      "metadata": {
        "id": "6832aipFIkSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 每个子列表代表一个词（或 token）的嵌入向量\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43, 0.15, 0.89],  # Your (x^1)\n",
        "        [0.55, 0.87, 0.66],  # journey (x^2)\n",
        "        [0.57, 0.85, 0.64],  # starts (x^3)\n",
        "        [0.22, 0.58, 0.33],  # with (x^4)\n",
        "        [0.77, 0.25, 0.10],  # one (x^5)\n",
        "        [0.05, 0.80, 0.55]   # step (x^6)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "BuoLin0OfTFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取第二个输入token作为查询向量（索引从0开始，所以inputs[1]是第二个元素）\n",
        "query = inputs[1]\n",
        "\n",
        "# 创建一个空张量，用于存储注意力分数，形状为(inputs.shape[0],)\n",
        "# inputs.shape[0]表示输入张量的第一个维度大小（即token的数量）\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "\n",
        "# 遍历输入张量中的每个token\n",
        "# i是当前token的索引，x_i是当前token的嵌入向量\n",
        "for i, x_i in enumerate(inputs):\n",
        "    # 计算当前token与查询向量的点积，作为注意力分数\n",
        "    # torch.dot()用于计算两个向量的点积\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n",
        "\n",
        "# 打印注意力分数\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "id": "YwqyOICWfXq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75478e36-9a87-4b8f-a742-69f990886657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算注意力权重：将注意力分数除以所有分数的总和，实现归一化\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "# 打印注意力权重\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "\n",
        "# 打印注意力权重的总和，验证是否归一化（理论上应为1）\n",
        "print(\"Sum:\", attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "id": "iQ_UGYyBJCDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0c63b5-1b74-4d0d-9991-3f35011b2142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(x):\n",
        "    \"\"\"\n",
        "    手动实现softmax函数\n",
        "    Args:\n",
        "        x: 输入张量\n",
        "    Returns:\n",
        "        经过softmax处理后的张量\n",
        "    \"\"\"\n",
        "    # 计算输入张量的指数值\n",
        "    exp_x = torch.exp(x)\n",
        "    # 计算指数值的和（按第0维求和）\n",
        "    sum_exp_x = exp_x.sum(dim=0)\n",
        "    # 返回softmax结果：每个元素的指数值除以总和\n",
        "    return exp_x / sum_exp_x\n",
        "\n",
        "# 使用手动实现的softmax函数计算注意力权重\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "# 打印注意力权重\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "\n",
        "# 打印注意力权重的总和，验证是否归一化（理论上应为1）\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "id": "RJSaL0v8JQvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd153865-fb2a-40da-902f-d91315613193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用PyTorch内置的softmax函数计算注意力权重\n",
        "# dim=0表示在第0维上进行softmax操作\n",
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "# 打印注意力权重\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "\n",
        "# 打印注意力权重的总和，验证是否归一化（理论上应为1）\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ],
      "metadata": {
        "id": "z4LQHSjQJczQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d031ffe-102f-47e3-e712-460184e59908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取第二个输入token作为查询向量（索引从0开始，所以inputs[1]是第二个元素）\n",
        "query = inputs[1]\n",
        "\n",
        "# 创建一个与查询向量形状相同的零张量，用于存储上下文向量\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "\n",
        "# 遍历输入张量中的每个token\n",
        "# i是当前token的索引，x_i是当前token的嵌入向量\n",
        "for i, x_i in enumerate(inputs):\n",
        "    # 将当前token的嵌入向量乘以对应的注意力权重，并累加到上下文向量中\n",
        "    context_vec_2 += attn_weights_2[i] * x_i\n",
        "\n",
        "# 打印计算得到的上下文向量\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "id": "lVu77QelJm2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13eefcca-a90f-46f0-89e5-58b20ece88ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.3.2 Computing attention weights for all input tokens"
      ],
      "metadata": {
        "id": "FF8Tt-nVKl-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个形状为(6, 6)的空张量，用于存储注意力分数\n",
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "# 遍历输入张量中的每个token（作为键，i是索引，x_i是嵌入向量）\n",
        "for i, x_i in enumerate(inputs):\n",
        "    # 遍历输入张量中的每个token（作为查询，j是索引，x_j是嵌入向量）\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        # 计算当前键（x_i）和查询（x_j）的点积，作为注意力分数\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "# 打印注意力分数矩阵\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "vh4d-_lmKVlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b6b8c2-356f-41f5-a01e-fc229b9ffb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用矩阵乘法计算自注意力分数矩阵\n",
        "# inputs.T 表示inputs的转置\n",
        "attn_scores = inputs @ inputs.T\n",
        "\n",
        "# 打印注意力分数矩阵\n",
        "print(attn_scores)\n",
        "# 确保前一个矩阵的列数 = 后一个矩阵的行数"
      ],
      "metadata": {
        "id": "TM0KnPMlKyrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c13e08e-811d-46a4-9988-18077316bfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对注意力分数矩阵进行softmax归一化\n",
        "# dim=-1表示在最后一个维度上进行softmax操作\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "# 打印归一化后的注意力权重矩阵\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "LPwvd5_pK_KP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ef338c-6a7e-4a9c-d634-70aa25197579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 手动计算第二行的和（假设这是softmax后的注意力权重）\n",
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "\n",
        "# 打印手动计算的第二行和\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "\n",
        "# 使用PyTorch的sum函数计算每行的和\n",
        "# dim=-1表示在最后一个维度（行）上求和\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ],
      "metadata": {
        "id": "l5F_3V6NLH-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4245cb-8302-48e3-f6a1-b3913de946e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用矩阵乘法计算所有上下文向量\n",
        "# attn_weights是注意力权重矩阵\n",
        "# inputs是原始输入嵌入向量\n",
        "all_context_vecs = attn_weights @ inputs\n",
        "\n",
        "# 打印计算得到的所有上下文向量\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "id": "WDbL69t3LoSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f62f1eb-2ea0-41ed-eca1-f2f7f1d1068b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ],
      "metadata": {
        "id": "VWIeKmrsLwf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db9b530-be0b-4b6e-ac8f-9ab339a68873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  3.4 Implementing self-attention with trainable weights"
      ],
      "metadata": {
        "id": "jm0MkQo7L04o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.4.1 Computing the attention weights step by step"
      ],
      "metadata": {
        "id": "QvM1ctFkMkSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取输入张量的第二个元素（索引从0开始，所以inputs[1]是第二个元素）\n",
        "x_2 = inputs[1]\n",
        "\n",
        "# 获取输入嵌入的维度（假设inputs是一个二维张量，inputs.shape[1]表示列数）\n",
        "d_in = inputs.shape[1]\n",
        "\n",
        "# 定义输出嵌入的维度\n",
        "d_out = 2"
      ],
      "metadata": {
        "id": "vA0AMlZhLzJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 初始化查询(Query)线性变换矩阵\n",
        "# d_in是输入维度，d_out是输出维度\n",
        "# requires_grad=False表示该矩阵不需要梯度更新\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "# 初始化键(Key)线性变换矩阵\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "# 初始化值(Value)线性变换矩阵\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "sg0Kk0GiMunu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算查询向量的线性变换结果\n",
        "# x_2是输入向量，W_query是查询线性变换矩阵\n",
        "query_2 = x_2 @ W_query\n",
        "\n",
        "# 计算键向量的线性变换结果\n",
        "# x_2是输入向量，W_key是键线性变换矩阵\n",
        "key_2 = x_2 @ W_key\n",
        "\n",
        "# 计算值向量的线性变换结果\n",
        "# x_2是输入向量，W_value是值线性变换矩阵\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "# 打印查询向量的线性变换结果\n",
        "print(query_2)"
      ],
      "metadata": {
        "id": "4nDfLMaXM0_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0270f870-1bbe-4485-b4e3-11a23eabd2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算所有输入向量的键表示\n",
        "# inputs是输入张量，W_key是键线性变换矩阵\n",
        "keys = inputs @ W_key\n",
        "\n",
        "# 计算所有输入向量的值表示\n",
        "# inputs是输入张量，W_value是值线性变换矩阵\n",
        "values = inputs @ W_value\n",
        "\n",
        "# 打印键表示的形状\n",
        "print(\"Keys.shape:\", keys.shape)\n",
        "\n",
        "# 打印值表示的形状\n",
        "print(\"Values.shape:\", values.shape)"
      ],
      "metadata": {
        "id": "ZfuwQsyNM6tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282b9cb7-6f41-4df6-ae92-7941966d3691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys.shape: torch.Size([6, 2])\n",
            "Values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取键张量中的第二个元素（索引从0开始，所以keys[1]是第二个元素）\n",
        "keys_2 = keys[1]\n",
        "\n",
        "# 计算查询向量和键向量的点积，作为注意力分数\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "\n",
        "# 打印计算得到的注意力分数\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "id": "qwP-3jShNHhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05139470-4055-4e40-b9ee-0700ec5987e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算单个查询向量与所有键向量的注意力分数\n",
        "# query_2是查询向量，keys.T是键张量的转置\n",
        "attn_scores_2 = query_2 @ keys.T\n",
        "\n",
        "# 打印计算得到的注意力分数\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "id": "_ggzDYF7PqMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14978f00-ff00-45b1-d861-8fe2b6b455df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取键向量的最后一个维度大小（即键的维度）\n",
        "d_k = keys.shape[-1]\n",
        "\n",
        "# 计算缩放的注意力权重\n",
        "# attn_scores_2是原始注意力分数\n",
        "# d_k**0.5是缩放因子，用于防止梯度消失\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / (d_k**0.5), dim=-1)\n",
        "\n",
        "# 打印计算得到的注意力权重\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "id": "Qg2xnAHBPzMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5bc269c-1073-43e0-afa0-6844e4fb175c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算上下文向量\n",
        "# attn_weights_2是注意力权重\n",
        "# values是值张量\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "\n",
        "# 打印计算得到的上下文向量\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "id": "vyHKQGpfQO2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc5605ae-99c7-4b29-d124-93b791ccfad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.2 Implementing a compact self-attention Python class"
      ],
      "metadata": {
        "id": "IDOfijhLTbp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  # __call__\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        # 初始化查询(Query)线性变换矩阵\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        # 初始化键(Key)线性变换矩阵\n",
        "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        # 初始化值(Value)线性变换矩阵\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 计算所有输入向量的键表示\n",
        "        keys = x @ self.W_key\n",
        "        # 计算所有输入向量的查询表示\n",
        "        queries = x @ self.W_query\n",
        "        # 计算所有输入向量的值表示\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        # 计算注意力分数\n",
        "        attn_scores = queries @ keys.T\n",
        "\n",
        "        # 计算缩放的注意力权重\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / (keys.shape[-1]**0.5),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # 计算上下文向量\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        # 返回上下文向量\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "SmL4WV-JTasz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 实例化自注意力模块\n",
        "# d_in是输入维度，d_out是输出维度\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "id": "JZJDceXgTnq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7f6ce8-cef6-4131-a9c0-029ed65ea205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # 定义查询(Query)线性层\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 定义键(Key)线性层\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 定义值(Value)线性层\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 计算所有输入向量的键表示\n",
        "        keys = self.W_key(x)\n",
        "        # 计算所有输入向量的查询表示\n",
        "        queries = self.W_query(x)\n",
        "        # 计算所有输入向量的值表示\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # 计算注意力分数\n",
        "        attn_scores = queries @ keys.T\n",
        "\n",
        "        # 计算缩放的注意力权重\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / (keys.shape[-1]**0.5),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # 计算上下文向量\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        # 返回上下文向量\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "8phheiFyT8kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "id": "waufOHzJUQ9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7fe3036-eae1-458a-a371-36b4c8f89a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 3.5 Hiding future words with causal attention"
      ],
      "metadata": {
        "id": "4CZn6R2gU1F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.5.1 Applying a causal attention mask"
      ],
      "metadata": {
        "id": "bI7Cx3WhU6W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算查询向量\n",
        "# sa_v2.W_query是查询线性层\n",
        "# inputs是输入张量\n",
        "queries = sa_v2.W_query(inputs)\n",
        "\n",
        "# 计算键向量\n",
        "# sa_v2.W_key是键线性层\n",
        "# inputs是输入张量\n",
        "keys = sa_v2.W_key(inputs)\n",
        "\n",
        "# 计算注意力分数\n",
        "# queries是查询矩阵，keys.T是键矩阵的转置\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "# 计算缩放的注意力权重\n",
        "# attn_scores是原始注意力分数\n",
        "# keys.shape[-1]**0.5是缩放因子，用于防止梯度消失\n",
        "attn_weights = torch.softmax(attn_scores / (keys.shape[-1]**0.5), dim=-1)\n",
        "\n",
        "# 打印计算得到的注意力权重\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "SrwYn8_XUdgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f73ba24-fca8-4a87-a104-ce6d2b5d50b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取注意力分数矩阵的行数（即上下文长度）\n",
        "context_length = attn_scores.shape[0]\n",
        "\n",
        "# 创建一个下三角掩码\n",
        "# torch.ones(context_length, context_length) 创建一个全1矩阵\n",
        "# torch.tril(...) 提取矩阵的下三角部分\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "\n",
        "# 打印创建的掩码\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "id": "hlrU4rWRWsVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078c9ef2-4a9e-456d-e6ce-fdac2153377a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将注意力权重与掩码相乘\n",
        "# attn_weights是原始注意力权重\n",
        "# mask_simple是下三角掩码\n",
        "masked_simple = attn_weights * mask_simple\n",
        "\n",
        "# 打印掩码后的注意力权重\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "id": "bezLWW0eXCEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e064a3cf-2a1b-47a5-ef9d-a0691acd8b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算每行的和\n",
        "# masked_simple是掩码后的注意力权重\n",
        "# dim=-1表示在最后一个维度（行）上求和\n",
        "# keepdim=True表示保持维度\n",
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
        "\n",
        "# 对掩码后的注意力权重进行归一化\n",
        "# masked_simple / row_sums 将每行的元素除以该行的和\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "\n",
        "# 打印归一化后的注意力权重\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "id": "yukCuASnXH63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940c9c47-70fe-46e3-b0d9-3faaea84e3f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个上三角掩码，对角线以上的元素为1\n",
        "# torch.ones(context_length, context_length) 创建一个全1矩阵\n",
        "# torch.triu(..., diagonal=1) 提取矩阵的上三角部分（不包括对角线）\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "\n",
        "# 将注意力分数矩阵中对应掩码为1的位置填充为负无穷\n",
        "# attn_scores.masked_fill(mask.bool(), -torch.inf) 将掩码位置填充为负无穷\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "\n",
        "# 打印掩码后的注意力分数\n",
        "print(masked)"
      ],
      "metadata": {
        "id": "y2oCmmotXOZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88803a58-1773-4a30-d020-4293f5a0eba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算缩放的注意力权重\n",
        "# masked是掩码后的注意力分数\n",
        "# keys.shape[-1]**0.5是缩放因子，用于防止梯度消失\n",
        "# dim=1表示在第二个维度（列）上进行softmax\n",
        "attn_weights = torch.softmax(masked / (keys.shape[-1]**0.5), dim=1)\n",
        "\n",
        "# 打印计算得到的注意力权重\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "Llf-di35XUi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03cc8f3-00a0-44f9-e658-1713dc1bacbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.5.2 Masking additional attention weights with dropout"
      ],
      "metadata": {
        "id": "E0QpQIjaXjYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "# torch.ones(*size)，用于生成指定形状的全 1 张量\n",
        "example = torch.ones(6, 6)\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "id": "fVMyF_V1Xf5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d57b65c-9357-4f27-adb2-7e2301b96468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "id": "RG9NEOSLXypA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0a28e0-4052-4a93-ea7d-44392b3dd6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.5.3 Implementing a compact causal attention class"
      ],
      "metadata": {
        "id": "8YwMn_oUYGJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 将两个输入张量沿第一个维度（批量维度）堆叠\n",
        "# dim=0表示沿第一个维度堆叠\n",
        "# torch.stack：新增维度进行堆叠，维度数会增加 1\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "\n",
        "# 打印堆叠后的张量形状\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "id": "ywQHttVtYFO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1bac01-313f-438c-876f-22b61cc765b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        # nn.Linear == y = x·W^T + b\n",
        "        # 定义查询(Query)线性层\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 定义键(Key)线性层\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 定义值(Value)线性层\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 定义Dropout层\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 注册一个缓冲区，存储因果掩码\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 获取输入张量的形状\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        # 计算所有输入向量的键表示\n",
        "        keys = self.W_key(x)\n",
        "        # 计算所有输入向量的查询表示\n",
        "        queries = self.W_query(x)\n",
        "        # 计算所有输入向量的值表示\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # 计算注意力分数\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "\n",
        "        # 应用因果掩码\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask[:num_tokens, :num_tokens].bool(), -torch.inf\n",
        "        )\n",
        "\n",
        "        # 计算缩放的注意力权重\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / (keys.shape[-1]**0.5),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # 应用Dropout\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # 计算上下文向量\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        # 返回上下文向量\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "bI_wTyGtZBIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 获取批量输入的上下文长度\n",
        "# batch.shape[1]获取第二个维度的大小（即上下文长度）\n",
        "context_length = batch.shape[1]\n",
        "\n",
        "# 实例化因果自注意力模块\n",
        "# d_in是输入维度，d_out是输出维度\n",
        "# context_length是上下文长度\n",
        "# 0.0表示不使用Dropout\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "# 计算因果自注意力模块的输出\n",
        "# batch是输入张量\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "# 打印输出张量的形状\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "id": "kEH2NtnxZQLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d886dff-1d4d-4ac5-a2f3-d9dffa20a94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  3.6 Extending single-head attention to multi-head attention"
      ],
      "metadata": {
        "id": "QgM8yPngsM1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.6.1 Stacking multiple single-head attention layers"
      ],
      "metadata": {
        "id": "9MJt-1RQse6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # 创建多个因果自注意力头\n",
        "        self.heads = nn.ModuleList([\n",
        "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "            for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 对每个自注意力头的输出进行拼接\n",
        "        # head(x) 调用每个自注意力头的前向传播方法\n",
        "        # torch.cat(..., dim=-1) 沿最后一个维度拼接\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "KlQHilzTZayF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 获取批量输入的上下文长度\n",
        "# batch是输入张量，假设形状为(2, 6, 3)\n",
        "# batch.shape[1]获取第二个维度的大小（即上下文长度）\n",
        "context_length = batch.shape[1]\n",
        "\n",
        "# 定义输入和输出维度\n",
        "d_in, d_out = 3, 2\n",
        "\n",
        "# 实例化多头自注意力模块\n",
        "# d_in是输入维度，d_out是输出维度\n",
        "# context_length是上下文长度\n",
        "# 0.0表示不使用Dropout\n",
        "# num_heads=2表示使用2个注意力头\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "# 计算多头自注意力模块的输出\n",
        "# batch是输入张量\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "# 打印输出张量及其形状\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "id": "60zL5Fxesshz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3e0937-39c3-4b59-ad86-a548f709d18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.6.2 Implementing multi-head attention with weight splits"
      ],
      "metadata": {
        "id": "90_C0Zi-txc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # 确保输出维度可以被注意力头数整除\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        # 定义查询(Query)、键(Key)、值(Value)线性层\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # 定义输出投影线性层\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "        # 定义Dropout层\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 注册因果掩码\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 获取输入张量的形状\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        # 计算查询、键、值\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # 重塑张量以适应多头注意力\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # 调整维度顺序\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # 计算注意力分数\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        # 应用因果掩码\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        # 计算缩放的注意力权重\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / (keys.shape[-1]**0.5),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # 应用Dropout\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # 计算上下文向量\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # 重塑张量并应用输出投影\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        # 返回上下文向量\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "Vk-dLwIws0jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个四维张量\n",
        "# 形状为 (1, 2, 3, 4)\n",
        "# 第一个维度：批量大小为1\n",
        "# 第二个维度：注意力头数为2\n",
        "# 第三个维度：token数量为3\n",
        "# 第四个维度：每个头的维度为4\n",
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
        "           [0.8993, 0.0390, 0.9268, 0.7388],\n",
        "           [0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "\n",
        "           [[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "           [0.4066, 0.2318, 0.4545, 0.9737],\n",
        "           [0.4606, 0.5159, 0.4220, 0.5786]]]])"
      ],
      "metadata": {
        "id": "So1DktMouA0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a @ a.transpose(2, 3))"
      ],
      "metadata": {
        "id": "ZeUpFGKauJrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5842b7-a15f-4862-c777-1e27cd3f7297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取第一个注意力头\n",
        "# a是输入张量，形状为(1, 2, 3, 4)\n",
        "# a[0, 0, :, :] 提取第一个样本、第一个头、所有标记、所有维度\n",
        "first_head = a[0, 0, :, :]\n",
        "\n",
        "# 计算第一个注意力头的自注意力分数\n",
        "# first_head @ first_head.T 计算第一个头的查询和键的点积\n",
        "first_res = first_head @ first_head.T\n",
        "\n",
        "# 打印第一个头的自注意力分数\n",
        "print(\"First head:\\n\", first_res)\n",
        "\n",
        "# 提取第二个注意力头\n",
        "# a[0, 1, :, :] 提取第一个样本、第二个头、所有标记、所有维度\n",
        "second_head = a[0, 1, :, :]\n",
        "\n",
        "# 计算第二个注意力头的自注意力分数\n",
        "# second_head @ second_head.T 计算第二个头的查询和键的点积\n",
        "second_res = second_head @ second_head.T\n",
        "\n",
        "# 打印第二个头的自注意力分数\n",
        "print(\"\\nSecond head:\\n\", second_res)"
      ],
      "metadata": {
        "id": "k2_qBm-ruL1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b45cd8-7925-4baa-c4e6-9aa0b9650042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First head:\n",
            " tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n",
            "\n",
            "Second head:\n",
            " tensor([[0.4391, 0.7003, 0.5903],\n",
            "        [0.7003, 1.3737, 1.0620],\n",
            "        [0.5903, 1.0620, 0.9912]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 获取批量输入的上下文长度和输入维度\n",
        "# batch是输入张量，假设形状为(2, 6, 3)\n",
        "# batch.shape[1]获取第二个维度的大小（即上下文长度）\n",
        "# batch.shape[2]获取第三个维度的大小（即输入维度）\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "\n",
        "# 定义输出维度\n",
        "d_out = 2\n",
        "\n",
        "# 实例化多头自注意力模块\n",
        "# d_in是输入维度，d_out是输出维度\n",
        "# context_length是上下文长度\n",
        "# 0.0表示不使用Dropout\n",
        "# num_heads=2表示使用2个注意力头\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "# 计算多头自注意力模块的输出\n",
        "# batch是输入张量\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "# 打印输出张量的形状\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "id": "gbFhZ2z7uWEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02897952-31cc-4ca3-ad25-c1206968dd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Implementing a GPT model from scratch to generate text"
      ],
      "metadata": {
        "id": "M-mz4p-jupzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Coding an LLM architecture"
      ],
      "metadata": {
        "id": "s3LHoTCx9gvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个名为GPT_CONFIG_124M的字典，用于配置一个1.24亿参数规模的GPT模型\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,  # 词汇表大小，即模型能够识别的不同单词或标记的数量\n",
        "    \"context_length\": 1024,  # 上下文长度，指模型在处理文本时能够考虑的最大单词数量\n",
        "    \"emb_dim\": 768,  # 嵌入维度，即每个单词或标记被转换为的向量空间的维度\n",
        "    \"n_heads\": 12,  # 注意力头的数量，即模型在处理注意力机制时使用的并行注意力头的数量\n",
        "    \"n_layers\": 12,  # 层数，即模型中Transformer层的数量\n",
        "    \"drop_rate\": 0.1,  # 丢弃率，即在训练过程中随机丢弃神经元的比例\n",
        "    \"qkv_bias\": False  # 查询-键-值偏置，指是否在计算注意力机制时使用偏置\n",
        "}"
      ],
      "metadata": {
        "id": "T7sW7upzutaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义一个简化版的GPT模型\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # 标记嵌入层，将输入标记转换为向量表示\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        # 位置嵌入层，为每个位置添加位置信息\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        # 嵌入层后的Dropout层，用于防止过拟合\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        # Transformer块序列，使用占位符类DummyTransformerBlock\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        # 最终的层归一化，使用占位符类DummyLayerNorm\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "        # 输出头，将处理后的向量映射回词汇表大小\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        # 获取输入张量的形状\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        # 计算标记嵌入\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        # 计算位置嵌入\n",
        "        pos_embeds = self.pos_emb(\n",
        "            torch.arange(seq_len, device=in_idx.device)\n",
        "        )\n",
        "        # 标记嵌入和位置嵌入相加\n",
        "        x = tok_embeds + pos_embeds\n",
        "        # 应用Dropout\n",
        "        x = self.drop_emb(x)\n",
        "        # 通过Transformer块\n",
        "        x = self.trf_blocks(x)\n",
        "        # 应用层归一化\n",
        "        x = self.final_norm(x)\n",
        "        # 计算输出logits\n",
        "        logits = self.out_head(x)\n",
        "        # 返回logits\n",
        "        return logits\n",
        "\n",
        "# 占位符类，实际应用中会被真实的Transformer块替换\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "# 占位符类，实际应用中会被真实的层归一化替换\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "U8t9YY3z920U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.Size([2, 4])\n",
        "# 第一个维度 2 表示批次大小（\n",
        "# 第二个维度 4 表示每个文本被分成 4 个 token\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "5UUxScBL-JBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37fca6b9-0b87-4a0c-a317-ad12205b4100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 实例化DummyGPTModel模型\n",
        "# GPT_CONFIG_124M是配置字典\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# 计算模型的输出\n",
        "# batch是输入张量\n",
        "logits = model(batch)\n",
        "\n",
        "# 打印输出张量的形状\n",
        "print(\"Output shape:\", logits.shape)\n",
        "\n",
        "# 打印输出张量\n",
        "# logits = [batch_size, seq_len, vocab_size]\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "U4MHXAuP-Wi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142e9f82-59b5-4e2b-a739-bc32a1e0edfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Normalizing activations with layer normalization"
      ],
      "metadata": {
        "id": "9j4wwCJ7-kxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 创建一个随机张量，形状为(2, 5)\n",
        "# 表示2个训练样本，每个样本有5个特征\n",
        "batch_example = torch.randn(2, 5)\n",
        "\n",
        "# 定义一个神经网络层序列\n",
        "# 包含一个线性层和一个ReLU激活函数\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "\n",
        "# 对输入张量进行前向传播\n",
        "out = layer(batch_example)\n",
        "\n",
        "# 打印输出结果\n",
        "print(out)"
      ],
      "metadata": {
        "id": "XecJClw6-gbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad1076b-f39c-4f42-b650-4a3b6f7d5beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算张量的均值，沿最后一个维度，保持维度\n",
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "\n",
        "# 计算张量的方差，沿最后一个维度，保持维度\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "\n",
        "# 打印均值\n",
        "print(\"Mean:\\n\", mean)\n",
        "\n",
        "# 打印方差\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "YTAzEIKl-0PS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb3e7d4-b498-4661-c131-ddc74fdd4e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对张量进行归一化处理\n",
        "# (out - mean) 减去均值\n",
        "# torch.sqrt(var) 计算方差的平方根\n",
        "out_norm = (out - mean) / torch.sqrt(var)\n",
        "\n",
        "# 重新计算归一化后的均值\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "\n",
        "# 重新计算归一化后的方差\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "\n",
        "# 打印归一化后的张量\n",
        "print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "\n",
        "# 打印归一化后的均值\n",
        "print(\"Mean:\\n\", mean)\n",
        "\n",
        "# 打印归一化后的方差\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "btpX0GvB-67X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a986123d-d64f-4f0f-d5bc-4e94ae6d8b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized layer outputs:\n",
            " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
            "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Mean:\n",
            " tensor([[9.9341e-09],\n",
            "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置PyTorch的打印选项，禁用科学计数法\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "# 打印均值\n",
        "print(\"Mean:\\n\", mean)\n",
        "\n",
        "# 打印方差\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "MF2NxM5p_Bt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40b8bc4-2319-4418-f202-4f11f5b78c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[    0.0000],\n",
            "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        # 数值稳定性参数\n",
        "        self.eps = 1e-5\n",
        "        # 缩放参数，初始化为全1\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        # 偏移参数，初始化为全0\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 计算输入张量的均值，沿最后一个维度，保持维度\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        # 计算输入张量的方差，沿最后一个维度，保持维度\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        # 归一化操作\n",
        "        # (x - mean) 减去均值\n",
        "        # torch.sqrt(var + self.eps) 计算方差的平方根，并加上数值稳定性参数\n",
        "        norm = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        # 应用缩放和偏移\n",
        "        return self.scale * norm + self.shift"
      ],
      "metadata": {
        "id": "Yj2lqssb_HeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化自定义的层归一化模块，嵌入维度为5\n",
        "ln = LayerNorm(emb_dim=5)\n",
        "\n",
        "# 对输入张量进行层归一化\n",
        "out_ln = ln(batch_example)\n",
        "\n",
        "# 计算归一化后的均值，沿最后一个维度，保持维度\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "\n",
        "# 计算归一化后的方差，沿最后一个维度，保持维度\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "\n",
        "# 打印归一化后的均值\n",
        "print(\"Mean:\\n\", mean)\n",
        "\n",
        "# 打印归一化后的方差\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "4h5rjh1k_Q2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd59f03b-b1a4-4b55-cf35-2393598a56f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[    -0.0000],\n",
            "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  4.3 Implementing a feed forward network with GELU activations"
      ],
      "metadata": {
        "id": "f6793NHG_Z_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "ICBthErt_X-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# 实例化GELU和ReLU激活函数\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "# 创建100个样本数据点，范围从-3到3\n",
        "x = torch.linspace(-3, 3, 100)\n",
        "y_gelu, y_relu = gelu(x), relu(x)\n",
        "plt.figure(figsize=(8, 3))\n",
        "# 遍历GELU和ReLU的输出及标签\n",
        "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    plt.plot(x, y)\n",
        "    plt.title(f\"{label} activation function\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(f\"{label}(x)\")\n",
        "    plt.grid(True)\n",
        "# 调整子图布局\n",
        "plt.tight_layout()\n",
        "# 显示图像\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-DG_BwNV_mkw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "2edf3b88-8d02-4e4a-970e-d6dcb195aac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # 定义前馈神经网络的层序列\n",
        "        self.layers = nn.Sequential(\n",
        "            # 第一个线性层，输入维度为emb_dim，输出维度为4*emb_dim\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            # GELU激活函数\n",
        "            GELU(),\n",
        "            # 第二个线性层，输入维度为4*emb_dim，输出维度为emb_dim\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 前向传播，将输入张量通过层序列\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "Xj-6GbWDAJj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化前馈神经网络模块，使用GPT_CONFIG_124M配置\n",
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "\n",
        "# 创建一个随机张量，形状为(2, 3, 768)\n",
        "# 表示批量大小为2，序列长度为3，嵌入维度为768\n",
        "x = torch.rand(2, 3, 768)\n",
        "\n",
        "# 对输入张量进行前向传播\n",
        "out = ffn(x)\n",
        "\n",
        "# 打印输出张量的形状\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "BMRuTid4AXVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "571166a9-a570-4a23-e0ba-21f2b2e7ba58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  4.4 Adding shortcut connections"
      ],
      "metadata": {
        "id": "YgIBf7pgAfFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes, use_shortcut):\n",
        "        super().__init__()\n",
        "        # 是否使用残差连接\n",
        "        self.use_shortcut = use_shortcut\n",
        "        # 定义神经网络的层列表\n",
        "        self.layers = nn.ModuleList([\n",
        "            # 第一层：线性层 + GELU激活函数\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "            # 第二层：线性层 + GELU激活函数\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "            # 第三层：线性层 + GELU激活函数\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "            # 第四层：线性层 + GELU激活函数\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "            # 第五层：线性层 + GELU激活函数\n",
        "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 遍历每一层\n",
        "        for layer in self.layers:\n",
        "            # 计算当前层的输出\n",
        "            layer_output = layer(x)\n",
        "            # 如果使用残差连接且输入和输出形状一致\n",
        "            if self.use_shortcut and x.shape == layer_output.shape:\n",
        "                # 应用残差连接，输入加上当前层的输出\n",
        "                x = x + layer_output\n",
        "            else:\n",
        "                # 否则，直接使用当前层的输出\n",
        "                x = layer_output\n",
        "        # 返回最终的输出\n",
        "        return x"
      ],
      "metadata": {
        "id": "IR0d6mdHAdpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义神经网络的层大小\n",
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "\n",
        "# 创建一个样本输入张量\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 实例化不使用残差连接的深度神经网络模型\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes, use_shortcut=False\n",
        ")"
      ],
      "metadata": {
        "id": "Z0joUA0MAsXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def print_gradients(model, x):\n",
        "    # 前向传播，计算模型的输出\n",
        "    output = model(x)\n",
        "    # 定义目标张量\n",
        "    target = torch.tensor([[0.]])\n",
        "    # 定义均方误差损失函数\n",
        "    loss = nn.MSELoss()\n",
        "    # 计算损失\n",
        "    loss = loss(output, target)\n",
        "    # 反向传播，计算梯度\n",
        "    loss.backward()\n",
        "\n",
        "    # 遍历模型的所有参数\n",
        "    for name, param in model.named_parameters():\n",
        "        # 如果参数名称包含'weight'\n",
        "        if 'weight' in name:\n",
        "            # 打印参数的梯度均值\n",
        "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "YWGjKRreA4fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_gradients(model_without_shortcut, sample_input)"
      ],
      "metadata": {
        "id": "aJvXpA4vBCU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ba6fb1-d76d-46ea-f64b-6d3fc9b30501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
            "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
            "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
            "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
            "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 实例化使用残差连接的深度神经网络模型\n",
        "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes, use_shortcut=True\n",
        ")\n",
        "\n",
        "# 计算模型的梯度并打印参数的梯度均值\n",
        "print_gradients(model_with_shortcut, sample_input)"
      ],
      "metadata": {
        "id": "JFgWan7wBTeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9886a7e1-9ffb-44ed-d76c-af9eb807d9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
            "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
            "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
            "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5  Connecting attention and linear layers in a transformer block"
      ],
      "metadata": {
        "id": "PsfCueNVBc-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# import MultiHeadAttention\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # 定义多头注意力模块\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"]\n",
        "        )\n",
        "        # 定义前馈神经网络模块\n",
        "        self.ff = FeedForward(cfg)\n",
        "        # 定义层归一化模块\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        # 定义dropout模块\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 保存原始输入作为残差连接\n",
        "        shortcut = x\n",
        "        # 应用层归一化\n",
        "        x = self.norm1(x)\n",
        "        # 应用多头注意力\n",
        "        x = self.att(x)\n",
        "        # 应用dropout\n",
        "        x = self.drop_shortcut(x)\n",
        "        # 应用残差连接\n",
        "        x = x + shortcut\n",
        "\n",
        "        # 保存当前输入作为残差连接\n",
        "        shortcut = x\n",
        "        # 应用层归一化\n",
        "        x = self.norm2(x)\n",
        "        # 应用前馈神经网络\n",
        "        x = self.ff(x)\n",
        "        # 应用dropout\n",
        "        x = self.drop_shortcut(x)\n",
        "        # 应用残差连接\n",
        "        x = x + shortcut\n",
        "\n",
        "        # 返回最终的输出\n",
        "        return x"
      ],
      "metadata": {
        "id": "f7gbi-qEBb4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 创建一个随机张量，形状为(2, 4, 768)\n",
        "# 表示批量大小为2，序列长度为4，嵌入维度为768\n",
        "x = torch.rand(2, 4, 768)\n",
        "\n",
        "# 实例化Transformer块，使用GPT_CONFIG_124M配置\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "\n",
        "# 对输入张量进行前向传播\n",
        "output = block(x)\n",
        "\n",
        "# 打印输入张量的形状\n",
        "print(\"Input shape:\", x.shape)\n",
        "\n",
        "# 打印输出张量的形状\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "Xkar0FS4COJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f630d9a-83ce-4897-a6fd-30ace1598c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Coding the GPT model"
      ],
      "metadata": {
        "id": "naifOlW1CZ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # 定义词嵌入层\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        # 定义位置嵌入层\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        # 定义dropout层\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # 定义Transformer块序列\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "        # 定义最终的层归一化\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        # 定义输出头\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        # 获取输入的批量大小和序列长度\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        # 计算词嵌入\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        # 计算位置嵌入\n",
        "        pos_embeds = self.pos_emb(\n",
        "            torch.arange(seq_len, device=in_idx.device)\n",
        "        )\n",
        "        # 词嵌入加上位置嵌入\n",
        "        x = tok_embeds + pos_embeds\n",
        "        # 应用dropout\n",
        "        x = self.drop_emb(x)\n",
        "        # 通过Transformer块序列\n",
        "        x = self.trf_blocks(x)\n",
        "        # 应用最终的层归一化\n",
        "        x = self.final_norm(x)\n",
        "        # 计算输出logits\n",
        "        logits = self.out_head(x)\n",
        "        # 返回logits\n",
        "        return logits"
      ],
      "metadata": {
        "id": "cQO_qsvsCYIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 实例化GPT模型，使用GPT_CONFIG_124M配置\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# 对输入批次进行前向传播\n",
        "out = model(batch)\n",
        "\n",
        "# 打印输入批次\n",
        "print(\"Input batch:\\n\", batch)\n",
        "\n",
        "# 打印输出形状\n",
        "print(\"Output shape:\", out.shape)\n",
        "\n",
        "# 打印输出结果\n",
        "print(out)"
      ],
      "metadata": {
        "id": "JDf02z7mClog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f7d4dc-bf56-434d-fd59-c4c2f64e2482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
            "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
            "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
            "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
            "\n",
            "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
            "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
            "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
            "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 计算模型的总参数数量\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# 打印总参数数量\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "G63ouwWmCse5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51734594-c95c-489e-c855-6fb435139c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163,009,536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 打印词嵌入层的权重形状\n",
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "\n",
        "# 打印输出层的权重形状\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "id": "-eEeaZOmC4fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c4cbb5-0a24-481f-a76c-b8739dc9c0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 计算考虑权重绑定后的可训练参数数量\n",
        "total_params_gpt2 = (\n",
        "    total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        ")\n",
        "\n",
        "# 打印结果\n",
        "print(f\"Number of trainable parameters \"\n",
        "      f\"considering weight tying: {total_params_gpt2:,}\")"
      ],
      "metadata": {
        "id": "yskcA-xMC-bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60a28d6-92bc-4b68-b890-13e9c9173f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 124,412,160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 计算模型的总大小（字节）\n",
        "total_size_bytes = total_params * 4\n",
        "\n",
        "# 将总大小转换为兆字节\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "\n",
        "# 打印模型的总大小\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "3DhKkaipDIeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005b249e-85b3-4d0a-acbb-8b888480d6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 621.83 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7  Generating text"
      ],
      "metadata": {
        "id": "Z3cLZjP0DRZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # 裁剪当前上下文，使其不超过支持的上下文大小\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            # 获取模型的输出logits\n",
        "            logits = model(idx_cond)\n",
        "        # 聚焦于最后一个时间步\n",
        "        logits = logits[:, -1, :]\n",
        "        # 将logits转换为概率\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        # 采样下一个token的索引\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "        # 将采样的token索引添加到运行序列中\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    # 返回生成的序列\n",
        "    return idx"
      ],
      "metadata": {
        "id": "ycPDwPNFDPy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 定义起始文本\n",
        "start_context = \"Hello, I am\"\n",
        "\n",
        "# 使用分词器对起始文本进行编码\n",
        "encoded = tokenizer.encode(start_context)\n",
        "\n",
        "# 打印编码结果\n",
        "print(\"encoded:\", encoded)\n",
        "\n",
        "# 将编码结果转换为张量，并添加批次维度\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "# 打印张量的形状\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
      ],
      "metadata": {
        "id": "0MbxN4A0DdA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2526ac9-2076-4568-fdb4-4efafea6899a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 将模型设置为评估模式，禁用dropout\n",
        "model.eval()\n",
        "\n",
        "# 使用简单的文本生成函数生成新的文本序列\n",
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=6,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# 打印生成的序列\n",
        "print(\"Output:\", out)\n",
        "\n",
        "# 打印生成的序列长度\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "id": "oNeTgrCmDk8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89517651-0dff-4e99-ec8e-12962aec066a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Output length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 将生成的张量转换为列表，并移除批次维度\n",
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "\n",
        "# 打印解码后的文本\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "NR06CA_8DuQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580bba21-d9cf-4ff0-d8f8-aeb086abefbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Pretraining on unlabeled data"
      ],
      "metadata": {
        "id": "yWZVe8EpD112"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1  Evaluating generative text models"
      ],
      "metadata": {
        "id": "BHT12MQ9D8zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.1 Using GPT to generate text"
      ],
      "metadata": {
        "id": "xvYKsF-PEBJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# from chapter04 import GPTModel\n",
        "\n",
        "# 定义GPT模型的配置\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 实例化GPT模型，使用GPT_CONFIG_124M配置\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# 将模型设置为评估模式，禁用dropout\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "9MpakNcLD0MS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1531d8da-5838-4b0d-85f2-b830ea601b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "# from chapter04 import generate_text_simple\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    # 将文本编码为token索引\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    # 转换为张量并添加批次维度\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    # 移除批次维度\n",
        "    flat = token_ids.squeeze(0)\n",
        "    # 将token索引解码为文本\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "# 定义起始文本\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "# 加载GPT-2的分词器\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 生成新的文本序列\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# 将生成的token索引转换回文本并打印\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "tHn2oA7jEWHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e7fc351-0bfb-4b5f-cda6-493fa1e0e01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.2  Calculating the text generation loss"
      ],
      "metadata": {
        "id": "HrsiJk3yEllq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "             [40,    1107, 588]])   #  \"I really like\"]"
      ],
      "metadata": {
        "id": "4VfrhrbHEn9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "             [1107, 588, 11311]])  #  \" really like chocolate\"]"
      ],
      "metadata": {
        "id": "p_6V1rkrEuta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 禁用梯度跟踪，提高性能并减少内存占用\n",
        "with torch.no_grad():\n",
        "    # 获取模型的输出logits\n",
        "    logits = model(inputs)\n",
        "    # 将logits转换为概率分布\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    # 打印概率分布的形状\n",
        "    print(probs.shape)"
      ],
      "metadata": {
        "id": "Dh5PaiaPE3VO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3463859a-f908-4815-cb9a-2402d55227c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 从概率分布中获取每个位置的最可能token索引\n",
        "token_ids = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "# 打印token索引\n",
        "print(\"Token IDs:\\n\", token_ids)"
      ],
      "metadata": {
        "id": "4Kuz3GtIE_bJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ea96cc-2466-4f51-8270-0329aae9b062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[16657],\n",
            "         [  339],\n",
            "         [42826]],\n",
            "\n",
            "        [[49906],\n",
            "         [29669],\n",
            "         [41751]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 打印目标批次的第一个样本\n",
        "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "\n",
        "# 打印生成的token索引的第一个样本\n",
        "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ],
      "metadata": {
        "id": "CHx09veMFHZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98648b57-80b9-4fba-dfaf-8ec40800a406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets batch 1:  effort moves you\n",
            "Outputs batch 1:  Armed heNetflix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义文本索引\n",
        "text_idx = 0\n",
        "\n",
        "# 计算目标token在概率分布中的概率值\n",
        "target_probas_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "\n",
        "# 打印结果\n",
        "print(\"Text 1:\", target_probas_1)\n",
        "\n",
        "# 定义文本索引\n",
        "text_idx = 1\n",
        "\n",
        "# 计算目标token在概率分布中的概率值\n",
        "target_probas_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "\n",
        "# 打印结果\n",
        "print(\"Text 2:\", target_probas_2)"
      ],
      "metadata": {
        "id": "J-KOqYkjFNWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af94f02-e81c-4cfb-e287-220a76fea1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
            "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 计算目标token概率的对数\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "\n",
        "# 打印结果\n",
        "print(log_probas)"
      ],
      "metadata": {
        "id": "Q9PEZM36KPaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ba97c4-2241-4257-fd00-64cb791a1ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算对数概率的平均值\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "\n",
        "# 打印结果\n",
        "print(avg_log_probas)"
      ],
      "metadata": {
        "id": "uY-pBo7yK0rB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c130aa-4802-4a5b-ff33-e3a39ac48c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-10.7940)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将平均对数概率取反\n",
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "\n",
        "# 打印结果\n",
        "print(neg_avg_log_probas)"
      ],
      "metadata": {
        "id": "Xt_KzyKXLNqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e0eadc-2528-4421-d865-3ae86e11e597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7940)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logits shape:\", logits.shape)\n",
        "print(\"Targets shape:\", targets.shape)"
      ],
      "metadata": {
        "id": "NjvkmlYYLVA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b46f71-9112-4872-8d50-b27f3ca2169d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 3, 50257])\n",
            "Targets shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 将logits张量展平\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "\n",
        "# 将targets张量展平\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "# 打印展平后的logits形状\n",
        "print(\"Flattened logits:\", logits_flat.shape)\n",
        "\n",
        "# 打印展平后的targets形状\n",
        "print(\"Flattened targets:\", targets_flat.shape)"
      ],
      "metadata": {
        "id": "T-n5iq9xLYo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6190ab-4a27-4e23-f8d8-6c33c3dc01dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened logits: torch.Size([6, 50257])\n",
            "Flattened targets: torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算交叉熵损失\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "UvLEjQzQLgXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce67d165-2098-4d11-a9bd-078072b71d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7940)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.3  Calculating the training and validation set losses"
      ],
      "metadata": {
        "id": "XaRiLNAUOR1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义文件路径\n",
        "file_path = \"the-verdict.txt\"\n",
        "\n",
        "# 以只读模式打开文件，指定编码为UTF-8\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    # 读取文件内容\n",
        "    text_data = file.read()"
      ],
      "metadata": {
        "id": "5GpWRTzPLpJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算文本数据的字符总数\n",
        "total_characters = len(text_data)\n",
        "\n",
        "# 计算文本数据的token总数\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "# 打印字符总数\n",
        "print(\"Characters:\", total_characters)\n",
        "\n",
        "# 打印token总数\n",
        "print(\"Tokens:\", total_tokens)"
      ],
      "metadata": {
        "id": "yuw58i3lOdan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21663796-f7b0-4c16-e4af-5e8950f9812e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 20479\n",
            "Tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义训练集比例\n",
        "train_ratio = 0.90\n",
        "\n",
        "# 计算分割索引\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "\n",
        "# 分割为训练集和验证集\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ],
      "metadata": {
        "id": "mVHuEoahOkRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from chapter02 import create_dataloader_v1\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 创建训练集的数据加载器\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# 创建验证集的数据加载器\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "QPe6PdvmOqVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印训练集数据加载器的批次形状\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "# 打印验证集数据加载器的批次形状\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "mVALvrljPAfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f3c577-4ca2-4f2d-c0e4-a518339d0a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    # 将输入数据和目标数据移动到指定设备（如GPU）\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    # 获取模型的输出logits\n",
        "    logits = model(input_batch)\n",
        "\n",
        "    # 计算交叉熵损失\n",
        "    loss = torch.nn.functional.cross_entropy(\n",
        "        logits.flatten(0, 1),  # 展平logits张量\n",
        "        target_batch.flatten()  # 展平目标张量\n",
        "    )\n",
        "\n",
        "    # 返回损失\n",
        "    return loss"
      ],
      "metadata": {
        "id": "nR9xDf1DPIPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    # 初始化总损失\n",
        "    total_loss = 0\n",
        "\n",
        "    # 处理空数据加载器的情况\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    # 确定要处理的批次数量\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    # 遍历数据加载器中的批次\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            # 计算单个批次的损失\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            # 累加损失\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # 返回平均损失\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "Kiv_cbpwf-o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 检查是否有CUDA支持的GPU，否则使用CPU\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 将模型移动到指定设备\n",
        "model.to(device)\n",
        "\n",
        "# 禁用梯度跟踪，提高性能并减少内存占用\n",
        "with torch.no_grad():\n",
        "    # 计算训练集的损失\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    # 计算验证集的损失\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "# 打印训练集损失\n",
        "print(\"Training loss:\", train_loss)\n",
        "# 打印验证集损失\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "id": "toSzmlBLgIi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0fe577-3821-4785-f8a4-b8fed45d9dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987583584255642\n",
            "Validation loss: 10.98110580444336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2  Training an LLM"
      ],
      "metadata": {
        "id": "3Q-KF-FXYqVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(\n",
        "    model, train_loader, val_loader,\n",
        "    optimizer, device, num_epochs,\n",
        "    eval_freq, eval_iter, start_context, tokenizer\n",
        "):\n",
        "    # 初始化损失列表和token统计\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # 主训练循环\n",
        "    for epoch in range(num_epochs):\n",
        "        # 设置模型为训练模式\n",
        "        model.train()\n",
        "        # 遍历训练集\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            # 更新全局步数\n",
        "            global_step += 1\n",
        "            # 重置梯度\n",
        "            optimizer.zero_grad()\n",
        "            # 计算损失\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            # 反向传播\n",
        "            loss.backward()\n",
        "            # 更新模型参数\n",
        "            optimizer.step()\n",
        "            # 更新已处理的token数\n",
        "            tokens_seen += input_batch.numel()\n",
        "\n",
        "            # 定期评估模型\n",
        "            if global_step % eval_freq == 0:\n",
        "                # 评估模型\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter\n",
        "                )\n",
        "                # 记录损失\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                # 打印损失\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, \"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # 生成并打印示例文本\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    # 返回训练历史\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "iCAsa56bgQPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    # 禁用梯度计算，提高性能并减少内存占用\n",
        "    with torch.no_grad():\n",
        "        # 计算训练集的损失\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        # 计算验证集的损失\n",
        "        val_loss = calc_loss_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "    # 设置模型为评估模式\n",
        "    model.eval()\n",
        "\n",
        "    # 返回训练集和验证集的损失\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "dnDTitZpY6Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    # 设置模型为评估模式\n",
        "    model.eval()\n",
        "\n",
        "    # 获取模型的上下文大小\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # 将起始文本编码为token ID\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "\n",
        "    # 禁用梯度计算\n",
        "    with torch.no_grad():\n",
        "        # 生成文本\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model,\n",
        "            idx=encoded,\n",
        "            max_new_tokens=50,\n",
        "            context_size=context_size\n",
        "        )\n",
        "\n",
        "    # 将token ID解码为文本\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    # 打印生成的文本（去除换行符）\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))\n",
        "\n",
        "    # 设置模型为训练模式\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "B7HBOCA4ZH-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 初始化模型\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# 将模型移动到指定设备（如GPU）\n",
        "model.to(device)\n",
        "\n",
        "# 初始化优化器\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0004,\n",
        "    weight_decay=0.1\n",
        ")\n",
        "\n",
        "# 定义训练轮次\n",
        "num_epochs = 10\n",
        "\n",
        "# 开始训练模型\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs=num_epochs,\n",
        "    eval_freq=5,\n",
        "    eval_iter=5,\n",
        "    start_context=\"Every effort moves you\",\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "6NT7RVSGZSsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad8d03a-c388-4897-8e04-382a893479d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.831, Val loss 9.968\n",
            "Ep 1 (Step 000005): Train loss 7.981, Val loss 8.328\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.776, Val loss 7.069\n",
            "Ep 2 (Step 000015): Train loss 6.076, Val loss 6.575\n",
            "Every effort moves you, and, and, and, and, and, and, and. \", and, and, and, and, and, and the, and, and, and, and, and, and, and, and, and, and\n",
            "Ep 3 (Step 000020): Train loss 5.495, Val loss 6.632\n",
            "Ep 3 (Step 000025): Train loss 5.154, Val loss 6.349\n",
            "Every effort moves you                                                  \n",
            "Ep 4 (Step 000030): Train loss 4.532, Val loss 6.341\n",
            "Ep 4 (Step 000035): Train loss 3.964, Val loss 6.153\n",
            "Every effort moves you know it was, and I felt--I to the picture. \"Oh, I was a little--and, I had been, and I was his the, and I had been the, and of his pictures--and it's the of\n",
            "Ep 5 (Step 000040): Train loss 3.999, Val loss 6.133\n",
            "Every effort moves you know it was not that, and in a little the fact of the of a little: \"Yes, with a little of the fact, in the of his pictures--I had always--his, and of Jack's \"There, I was his\n",
            "Ep 6 (Step 000045): Train loss 3.312, Val loss 6.129\n",
            "Ep 6 (Step 000050): Train loss 2.501, Val loss 6.130\n",
            "Every effort moves you know,\" was not that my dear, and he had been the his painting.                                  \n",
            "Ep 7 (Step 000055): Train loss 2.239, Val loss 6.175\n",
            "Ep 7 (Step 000060): Train loss 1.490, Val loss 6.227\n",
            "Every effort moves you?\"  \"Yes--I glanced after him, so a little: \"There: make yourself comfortable--and here are the cigars you like.\"    \"I had been the honour being _mine_--because he didn't want\n",
            "Ep 8 (Step 000065): Train loss 1.298, Val loss 6.246\n",
            "Ep 8 (Step 000070): Train loss 0.836, Val loss 6.278\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back his head to look up at the honour being _mine_--oh, and in his\n",
            "Ep 9 (Step 000075): Train loss 0.847, Val loss 6.318\n",
            "Ep 9 (Step 000080): Train loss 0.463, Val loss 6.437\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with equanimity.        He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 10 (Step 000085): Train loss 0.517, Val loss 6.418\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    # 创建图形和子图\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # 绘制训练损失和验证损失随训练轮次的变化\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"--\", label=\"Validation loss\")\n",
        "\n",
        "    # 设置x轴标签\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    # 设置y轴标签\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    # 设置图例位置\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    # 设置x轴为整数刻度\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "    # 创建第二个x轴，共享y轴\n",
        "    ax2 = ax1.twiny()\n",
        "    # 绘制处理token数的不可见曲线，用于对齐刻度\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "    # 设置第二个x轴标签\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    # 自动调整子图布局\n",
        "    fig.tight_layout()\n",
        "    # 显示图形\n",
        "    plt.show()\n",
        "\n",
        "# 创建训练轮次的张量\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "\n",
        "# 调用plot_losses函数绘制损失曲线\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "1CgtUDOJZaSQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "8335d3c7-9889-4675-9038-24cdbfe952b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Decoding strategies to control randomness"
      ],
      "metadata": {
        "id": "-1J-OtuqfFFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 将模型移动到CPU\n",
        "model.to(\"cpu\")\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "2-6ZZyErfDQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea0fe73-b55e-40f2-9416-9b2fa5e4e686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取GPT-2分词器\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 生成文本\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# 将生成的token ID解码为文本并打印\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "hPrI9Sg8fYZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461d5e83-7966-47c7-c93e-f899c1125d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.1  Temperature scaling"
      ],
      "metadata": {
        "id": "LcjelCp9fpzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义词汇表\n",
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "# 定义反向词汇表\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}"
      ],
      "metadata": {
        "id": "Hxza83y7fh7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义下一个token的logits张量\n",
        "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])"
      ],
      "metadata": {
        "id": "S-S6FVhifvBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将logits转换为概率分布\n",
        "probs = torch.softmax(next_token_logits, dim=0)\n",
        "\n",
        "# 选择概率最高的token的索引\n",
        "next_token_id = torch.argmax(probs).item()\n",
        "\n",
        "# 将token索引转换为单词并打印\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "id": "7wMJzpe9f3Ra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac99700c-f5e8-44d4-ace3-518869c5de0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 从概率分布中随机选择一个token\n",
        "next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "# 将token索引转换为单词并打印\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "id": "KcGjVxVlf_72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf275002-5659-49bb-a976-ef3d4e7ee08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sampled_tokens(probs):\n",
        "    # 设置随机种子，确保结果可复现\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    # 从概率分布中采样1000个token\n",
        "    sample = [torch.multinomial(probs, num_samples=1).item() for _ in range(1000)]\n",
        "\n",
        "    # 计算每个token的频率\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "\n",
        "    # 打印每个token的频率和对应的单词\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
        "\n",
        "# 调用函数\n",
        "print_sampled_tokens(probs)"
      ],
      "metadata": {
        "id": "0BID4MPvgHs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d630fe7b-df84-44f5-c13f-5189399888c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71 x closer\n",
            "2 x every\n",
            "0 x effort\n",
            "544 x forward\n",
            "2 x inches\n",
            "1 x moves\n",
            "0 x pizza\n",
            "376 x toward\n",
            "4 x you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    # 对logits进行缩放\n",
        "    scaled_logits = logits / temperature\n",
        "    # 计算带温度参数的softmax概率分布\n",
        "    return torch.softmax(scaled_logits, dim=0)"
      ],
      "metadata": {
        "id": "y08QWUnghN3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义不同的温度参数，用于控制softmax的输出分布\n",
        "# 温度为1时接近原始分布，0.1时分布更集中（高置信度），5时分布更平坦（低置信度）\n",
        "temperatures = [1, 0.1, 5]\n",
        "\n",
        "# 对每个温度参数，使用带温度的softmax函数计算概率分布\n",
        "# scaled_probas将存储不同温度下的概率分布列表\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
        "\n",
        "# 生成x轴的位置坐标，用于绘制柱状图\n",
        "# x的长度等于词汇表的大小，每个位置对应一个词汇\n",
        "x = torch.arange(len(vocab))\n",
        "\n",
        "# 定义柱状图的宽度，用于控制不同温度下柱状图的间距\n",
        "bar_width = 0.15\n",
        "\n",
        "# 创建一个图形和子图对象，设置图形大小为5x3英寸\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "# 遍历每个温度参数及其索引\n",
        "for i, T in enumerate(temperatures):\n",
        "    # 绘制当前温度下的概率分布柱状图\n",
        "    # x + i * bar_width：计算每个温度下柱状图的x轴位置\n",
        "    # scaled_probas[i]：当前温度下的概率分布，作为柱状图的高度\n",
        "    # bar_width：柱状图的宽度\n",
        "    # label：为当前温度的柱状图添加图例标签\n",
        "    rects = ax.bar(\n",
        "        x + i * bar_width,\n",
        "        scaled_probas[i],\n",
        "        bar_width,\n",
        "        label=f\"Temperature = {T}\"\n",
        "    )\n",
        "\n",
        "# 设置y轴的标签为\"Probability\"，表示y轴显示的是概率值\n",
        "ax.set_ylabel('Probability')\n",
        "\n",
        "# 设置x轴的刻度位置，与x的长度对应\n",
        "ax.set_xticks(x)\n",
        "\n",
        "# 设置x轴的刻度标签为词汇表中的单词，旋转90度以避免重叠\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "\n",
        "# 显示图例，解释不同颜色柱状图对应的温度参数\n",
        "ax.legend()\n",
        "\n",
        "# 自动调整子图布局，确保所有元素都能正确显示\n",
        "plt.tight_layout()\n",
        "\n",
        "# 显示绘制的图形\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yj4qDgR3hYiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "8bbd0963-fb9f-44e1-ec1f-474533f78385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.2  Top-k sampling"
      ],
      "metadata": {
        "id": "LIxnquCXiIsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义要获取的top token数量\n",
        "top_k = 3\n",
        "\n",
        "# 获取概率最高的前k个token的logits和位置\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "\n",
        "# 打印概率最高的前k个token的logits\n",
        "print(\"Top logits:\", top_logits)\n",
        "\n",
        "# 打印概率最高的前k个token的位置\n",
        "print(\"Top positions:\", top_pos)"
      ],
      "metadata": {
        "id": "LClKsZB7hfXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b66185-dc9c-4c62-a5c1-9abdee180346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
            "Top positions: tensor([3, 7, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.where 是 PyTorch 中用于根据条件选择元素的函数\n",
        "# 使用torch.where函数将低于top_logits[-1]的logits设置为-∞\n",
        "new_logits = torch.where(\n",
        "    # 条件：logits小于top_logits中的最小值\n",
        "    condition=next_token_logits < top_logits[-1],\n",
        "    # 满足条件时，设置为-∞\n",
        "    input=torch.tensor(float('-inf')),\n",
        "    # 不满足条件时，保留原logits\n",
        "    other=next_token_logits\n",
        ")\n",
        "\n",
        "# 打印处理后的logits\n",
        "print(new_logits)"
      ],
      "metadata": {
        "id": "Q2mWu7S4iQ7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2eaddba-e70a-4b40-c35c-40688282739e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对处理后的logits应用softmax函数，生成概率分布\n",
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "\n",
        "# 打印生成的概率分布\n",
        "print(topk_probas)"
      ],
      "metadata": {
        "id": "WM1Tgdf0iWj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6477e59-2150-431f-f964-83e09ebd98cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.3 Modifying the text generation function"
      ],
      "metadata": {
        "id": "LDiCrxfuid0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    model, idx, max_new_tokens, context_size,\n",
        "    temperature=0.0, top_k=None, eos_id=None\n",
        "):\n",
        "    # 生成指定数量的新token\n",
        "    for _ in range(max_new_tokens):\n",
        "        # 截取最后context_size个token作为条件\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # 禁用梯度计算\n",
        "        with torch.no_grad():\n",
        "            # 获取模型的预测logits\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # 只关注最后一个时间步的预测\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # 如果指定了top_k，应用top_k采样\n",
        "        if top_k is not None:\n",
        "            # 获取top_k的logits和位置\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            # 获取top_k中的最小值\n",
        "            min_val = top_logits[:, -1]\n",
        "            # 将低于最小值的logits设置为-∞\n",
        "            logits = torch.where(\n",
        "                logits < min_val[:, None],\n",
        "                torch.tensor(float('-inf')).to(logits.device),\n",
        "                logits\n",
        "            )\n",
        "\n",
        "        # 如果温度大于0，应用温度缩放\n",
        "        if temperature > 0.0:\n",
        "            # 对logits进行温度缩放\n",
        "            logits = logits / temperature\n",
        "            # 转换为概率分布\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            # 从概率分布中采样\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            # 否则，使用贪婪解码\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # 如果生成了结束token，提前停止生成\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        # 将新生成的token添加到序列中\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    # 返回生成的序列\n",
        "    return idx"
      ],
      "metadata": {
        "id": "J5zmS9hYibrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 生成文本\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "# 将生成的token ID解码为文本并打印\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "hBoeWg76iq96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae34111c-2ad2-42d8-99fd-0f7b0226a9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you know I meant to a little wild--I was such struck by his last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4  Loading and saving model weights in PyTorch"
      ],
      "metadata": {
        "id": "SuLwtl70iz8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "zve4ktPGiwXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化GPT模型\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# 加载预训练的模型权重\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "YZlHah7Ei463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be35fe2-e4db-453d-c753-25935d1b6a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型和优化器的状态\n",
        "torch.save({\n",
        "    # 模型的状态字典\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    # 优化器的状态字典\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "}, \"model_and_optimizer.pth\")"
      ],
      "metadata": {
        "id": "gAYoO1QZjQDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载保存的模型和优化器状态\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
        "\n",
        "# 初始化GPT模型\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# 加载模型的状态字典\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "# 初始化优化器\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "\n",
        "# 加载优化器的状态字典\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "# 设置模型为训练模式\n",
        "model.train()"
      ],
      "metadata": {
        "id": "3_-0ksQhjXBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7f13b7-f976-411a-c7b6-9b1de41bcede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5  Loading pretrained weights from OpenAI"
      ],
      "metadata": {
        "id": "d9R1GbYLj39T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow>=2.15.0  tqdm>=4.66"
      ],
      "metadata": {
        "id": "GpECNcYGj0hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入urllib.request模块，用于处理URL请求\n",
        "import urllib.request\n",
        "\n",
        "# 定义要下载的文件的URL\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "\n",
        "# 从URL中提取文件名\n",
        "filename = url.split('/')[-1]\n",
        "\n",
        "# 下载文件并保存到本地\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "HLSuakjjj-Bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f896aca5-09e3-4745-cc2c-a4e16377985b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7d9186bed090>)"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 从gpt_download模块中导入download_and_load_gpt2函数\n",
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "# 下载并加载GPT-2模型的设置和参数\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\",  # 指定模型大小为124M\n",
        "    models_dir=\"gpt2\"    # 指定模型保存的目录为gpt2\n",
        ")"
      ],
      "metadata": {
        "id": "2sZ16r-ukFfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9797b6a-f6f7-47fe-f750-0cd96626d9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 75.8kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.30MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 98.8kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:27<00:00, 18.4MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 1.79MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.95MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.27MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "id": "sSduHwxwkMBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b36611-7c07-4649-c066-38e6a754caba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
      ],
      "metadata": {
        "id": "FuXMfndYkYJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7bfbc9-d5fe-4a08-d899-27a26a018442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embedding weight tensor dimensions: (50257, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "    # GPT-2 Small 模型配置\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    # GPT-2 Medium 模型配置\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    # GPT-2 Large 模型配置\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    # GPT-2 XL 模型配置\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ],
      "metadata": {
        "id": "c8q9wZ2ckffR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型名称\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "\n",
        "# 更新新的配置字典，添加指定模型的配置信息\n",
        "NEW_CONFIG.update(model_configs[model_name])"
      ],
      "metadata": {
        "id": "ucTyGBLhkl7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG.update({\"context_length\": 1024})"
      ],
      "metadata": {
        "id": "FeReezFnkrrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG.update({\"qkv_bias\": True})"
      ],
      "metadata": {
        "id": "ULClUfcukueT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用新的配置创建 GPT 模型\n",
        "# NEW_CONFIG\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "\n",
        "# 设置模型为评估模式\n",
        "gpt.eval()"
      ],
      "metadata": {
        "id": "bLcChPabkxOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ec1737-3d31-41eb-e6fa-9b51a4d7cf94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    # 检查左侧和右侧张量的形状是否一致\n",
        "    if left.shape != right.shape:\n",
        "        # 如果形状不一致，抛出 ValueError 异常\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "\n",
        "    # 将右侧张量转换为 Parameter 类型并返回\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "1Ip_-3c5k1IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    # 加载位置嵌入权重\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
        "    # 加载词嵌入权重\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
        "\n",
        "    # 遍历每个 Transformer 块\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        # 分割注意力权重为查询、键和值三部分\n",
        "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
        "        # 加载查询权重\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        # 加载键权重\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        # 加载值权重\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        # 分割注意力偏置为查询、键和值三部分\n",
        "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
        "        # 加载查询偏置\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        # 加载键偏置\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        # 加载值偏置\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        # 加载输出投影权重\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        # 加载输出投影偏置\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # 加载前馈网络第一层权重\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        # 加载前馈网络第一层偏置\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        # 加载前馈网络第二层权重\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        # 加载前馈网络第二层偏置\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # 加载第一层归一化的缩放参数\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        # 加载第一层归一化的移位参数\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        # 加载第二层归一化的缩放参数\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        # 加载第二层归一化的移位参数\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    # 加载最终归一化的缩放参数\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    # 加载最终归一化的移位参数\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    # 加载输出头权重（权重绑定）\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "SiuTqh6cmRwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "id": "vopcdJzTmjMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37efb6ad-fcca-4a8e-c930-832c9accbceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子，确保结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 生成文本\n",
        "token_ids = generate(\n",
        "    model=gpt,  # 使用的模型\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),  # 输入文本的token ID\n",
        "    max_new_tokens=25,  # 生成的最大新token数\n",
        "    context_size=NEW_CONFIG[\"context_length\"],  # 上下文长度\n",
        "    top_k=50,  # top_k采样的k值\n",
        "    temperature=1.5  # 温度参数\n",
        ")\n",
        "\n",
        "# 将生成的token ID解码为文本并打印\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "32ldVU5Pml-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4eebbb-f944-4ff5-b7e4-dcfe7956f3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Fine-tuning for classification"
      ],
      "metadata": {
        "id": "CprsxBJHnbYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1  Different categories of fine-tuning"
      ],
      "metadata": {
        "id": "ANkYMYifLzwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2  Preparing the dataset"
      ],
      "metadata": {
        "id": "zLf1Up8EL31c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 定义数据集的 URL 和路径\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    # 检查数据文件是否已存在\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # 下载数据集\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # 解压数据集\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # 重命名文件并添加 .tsv 扩展名\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "# 调用函数下载并解压数据集\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "id": "VvnPRJCsmxQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a69f1c2-edac-48df-980e-a520de603471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 读取数据集\n",
        "df = pd.read_csv(\n",
        "    data_file_path,  # 数据集文件路径\n",
        "    sep=\"\\t\",        # 分隔符为制表符\n",
        "    header=None,     # 文件没有表头\n",
        "    names=[\"Label\", \"Text\"]  # 指定列名\n",
        ")\n",
        "\n",
        "# 显示 DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "6BFtfmhyMFRO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "5257ac50-a3fe-4b3a-dc8f-4a6a21827900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad7db5da-edca-46fb-9431-ff79b251c71d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad7db5da-edca-46fb-9431-ff79b251c71d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad7db5da-edca-46fb-9431-ff79b251c71d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad7db5da-edca-46fb-9431-ff79b251c71d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9709df0f-1a68-4a06-832f-cf11673932c9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9709df0f-1a68-4a06-832f-cf11673932c9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9709df0f-1a68-4a06-832f-cf11673932c9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_008a95a8-117a-4a67-a5a3-e7310d8459b8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_008a95a8-117a-4a67-a5a3-e7310d8459b8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "W9Km1CGDMOfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997e60ac-921e-4df0-bb27-81eca684b576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_balanced_dataset(df):\n",
        "    # 计算 \"spam\" 类别的样本数量\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # 随机采样 \"ham\" 类别的样本，数量与 \"spam\" 类别相同\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
        "        num_spam,  # 采样数量\n",
        "        random_state=123  # 随机种子，确保结果可复现\n",
        "    )\n",
        "\n",
        "    # 合并采样后的 \"ham\" 子集和 \"spam\" 类别\n",
        "    balanced_df = pd.concat([\n",
        "        ham_subset,  # 采样后的 \"ham\" 子集\n",
        "        df[df[\"Label\"] == \"spam\"]  # \"spam\" 类别\n",
        "    ])\n",
        "\n",
        "    # 返回平衡的数据集\n",
        "    return balanced_df\n",
        "\n",
        "# 创建平衡的数据集\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "\n",
        "# 打印每个类别的样本数量\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "sSLLGqgYMR9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c95162-e26a-4b2a-dd48-562a1712b1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将 \"Label\" 列中的类别标签映射为数字\n",
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "i_Gbt5FoMY01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # 打乱数据集\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # 计算分割索引\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # 分割数据集\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    # 返回分割后的数据集\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "# 分割数据集\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
      ],
      "metadata": {
        "id": "M1w6CG-WMh_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将训练集保存为 CSV 文件\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "\n",
        "# 将验证集保存为 CSV 文件\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "\n",
        "# 将测试集保存为 CSV 文件\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "BOGSa5Z9MotG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3  Creating data loaders"
      ],
      "metadata": {
        "id": "cOuHFzsxMvKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# 获取 GPT-2 的分词器\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 编码特殊文本 `<|endoftext|>`\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "id": "pzeQw2RcMuHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fe5378-f520-4ad4-8844-f3d74bbf8bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        # 读取 CSV 文件\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # 编码文本\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        # 确定最大长度\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "\n",
        "        # 截断和填充文本\n",
        "        self.encoded_texts = [\n",
        "            encoded_text[:self.max_length]\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 获取编码后的文本和标签\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        # 返回数据集的大小\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        # 计算最长编码文本的长度\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ],
      "metadata": {
        "id": "6oIjSfy5M2tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",  # 训练集 CSV 文件路径\n",
        "    max_length=None,       # 最大长度（None 表示自动计算）\n",
        "    tokenizer=tokenizer    # 分词器\n",
        ")"
      ],
      "metadata": {
        "id": "f-NVHCvtNBS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "id": "wzEkt2HCNIrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6367a049-a45d-49ff-8f13-829c13f81dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建验证集数据集\n",
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",  # 验证集 CSV 文件路径\n",
        "    max_length=train_dataset.max_length,  # 最大长度与训练集一致\n",
        "    tokenizer=tokenizer  # 分词器\n",
        ")\n",
        "\n",
        "# 创建测试集数据集\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",  # 测试集 CSV 文件路径\n",
        "    max_length=train_dataset.max_length,  # 最大长度与训练集一致\n",
        "    tokenizer=tokenizer  # 分词器\n",
        ")"
      ],
      "metadata": {
        "id": "joko52grNK5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 设置参数\n",
        "num_workers = 0  # 工作进程数，0 表示不使用多进程\n",
        "batch_size = 8  # 批大小\n",
        "torch.manual_seed(123)  # 设置随机种子，确保结果可复现\n",
        "\n",
        "# 创建训练集数据加载器\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,  # 训练集数据集\n",
        "    batch_size=batch_size,  # 批大小\n",
        "    shuffle=True,  # 打乱数据\n",
        "    num_workers=num_workers,  # 工作进程数\n",
        "    drop_last=True  # 丢弃最后一个不完整的批次\n",
        ")\n",
        "\n",
        "# 创建验证集数据加载器\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,  # 验证集数据集\n",
        "    batch_size=batch_size,  # 批大小\n",
        "    num_workers=num_workers,  # 工作进程数\n",
        "    drop_last=False  # 不丢弃最后一个不完整的批次\n",
        ")\n",
        "\n",
        "# 创建测试集数据加载器\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,  # 测试集数据集\n",
        "    batch_size=batch_size,  # 批大小\n",
        "    num_workers=num_workers,  # 工作进程数\n",
        "    drop_last=False  # 不丢弃最后一个不完整的批次\n",
        ")"
      ],
      "metadata": {
        "id": "oTdnmz92NR1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 遍历训练集数据加载器\n",
        "for input_batch, target_batch in train_loader:\n",
        "    pass  # 占位符，实际代码中可以处理批次数据\n",
        "\n",
        "# 打印输入批次的维度\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "\n",
        "# 打印标签批次的维度\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ],
      "metadata": {
        "id": "A9otRapsNZKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bfdb98-4ac0-4b4d-a940-2a8998bb78bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch dimensions: torch.Size([8, 120])\n",
            "Label batch dimensions torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印训练集的批次数量\n",
        "print(f\"{len(train_loader)} training batches\")\n",
        "\n",
        "# 打印验证集的批次数量\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "\n",
        "# 打印测试集的批次数量\n",
        "print(f\"{len(test_loader)} test batches\")"
      ],
      "metadata": {
        "id": "J7HrmhujNeaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "519d361f-c74f-4f0f-af0c-f0239e8e0b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4  Initializing a model with pretrained weights"
      ],
      "metadata": {
        "id": "gJA0f_egNnGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 选择的模型\n",
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "# 输入提示\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "# 基础配置\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,  # 词汇表大小\n",
        "    \"context_length\": 1024,  # 上下文长度\n",
        "    \"drop_rate\": 0.0,  # Dropout 率\n",
        "    \"qkv_bias\": True  # 查询-键-值偏置\n",
        "}\n",
        "\n",
        "# 模型配置\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# 更新基础配置\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ],
      "metadata": {
        "id": "lFBTSxjBNl4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从自定义模块导入函数\n",
        "from gpt_download import download_and_load_gpt2\n",
        "# from chapter05 import GPTModel, load_weights_into_gpt\n",
        "\n",
        "# 解析模型大小\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "\n",
        "# 下载并加载 GPT-2 模型\n",
        "settings, params = download_and_load_gpt2(\n",
        "  model_size=model_size, models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "# 初始化模型\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "\n",
        "# 加载权重\n",
        "load_weights_into_gpt(model, params)\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cdRYlg7GNw0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8f4d68-fa96-469f-c4b0-a9d7eab799bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 从自定义模块导入函数\n",
        "# from chapter04 import generate_text_simple\n",
        "# from chapter05 import text_to_token_ids, token_ids_to_text\n",
        "\n",
        "# 输入文本\n",
        "text_1 = \"Every effort moves you\"\n",
        "\n",
        "# 生成文本\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,  # 使用的模型\n",
        "    idx=text_to_token_ids(text_1, tokenizer),  # 输入文本的 token ID\n",
        "    max_new_tokens=15,  # 生成的最大新 token 数\n",
        "    context_size=BASE_CONFIG[\"context_length\"]  # 上下文长度\n",
        ")\n",
        "\n",
        "# 将生成的 token ID 解码为文本并打印\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "a-w4Xtq0N4i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24e8e41-5f37-46c6-eaf2-dd4cbf8d2b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward.\n",
            "\n",
            "The first step is to understand the importance of your work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输入文本\n",
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\\n\"\n",
        "    \"You are a winner you have been specially selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "# 生成文本\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,  # 使用的模型\n",
        "    idx=text_to_token_ids(text_2, tokenizer),  # 输入文本的 token ID\n",
        "    max_new_tokens=23,  # 生成的最大新 token 数\n",
        "    context_size=BASE_CONFIG[\"context_length\"]  # 上下文长度\n",
        ")\n",
        "\n",
        "# 将生成的 token ID 解码为文本并打印\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "fg6eA-P1ZgPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa6be86-8beb-407d-f55d-aa280345ddd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the following text 'spam'? Answer with 'yes' or 'no':\n",
            "You are a winner you have been specially selected to receive $1000 cash or a $2000 award.\n",
            "\n",
            "You have been specially selected to receive $1000 cash or a $2000 award. You have been specially selected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Adding a classification head"
      ],
      "metadata": {
        "id": "hru2311ofnjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzGufsZ4cnvS",
        "outputId": "7c6963fc-18bf-481c-b911-2933e38116ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTModel(\n",
            "  (tok_emb): Embedding(50257, 768)\n",
            "  (pos_emb): Embedding(1024, 768)\n",
            "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
            "  (trf_blocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm()\n",
            "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 冻结模型的所有参数\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "OnitUl0Fft3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 分类类别数量\n",
        "num_classes = 2\n",
        "\n",
        "# 添加输出层\n",
        "model.out_head = torch.nn.Linear(\n",
        "    in_features=BASE_CONFIG[\"emb_dim\"],  # 输入特征维度（嵌入维度）\n",
        "    out_features=num_classes  # 输出特征维度（类别数量）\n",
        ")"
      ],
      "metadata": {
        "id": "T4Z433vCiTsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 解冻最后一个 Transformer 块的参数\n",
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 解冻最终归一化层的参数\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "0yBScm7mlQ-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 编码输入文本\n",
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "\n",
        "# 转换为 PyTorch 张量，并添加批次维度\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "\n",
        "# 打印输入的形状\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Inputs dimensions:\", inputs.shape)"
      ],
      "metadata": {
        "id": "vDe1lDhVn-8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35eca8c6-5914-4b14-bc94-bcc7a8e1970f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[5211,  345,  423,  640]])\n",
            "Inputs dimensions: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 在不计算梯度的上下文中进行推理\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)  # 使用模型进行推理\n",
        "\n",
        "# 打印输出结果和维度\n",
        "print(\"Outputs:\", outputs)\n",
        "print(\"Outputs dimensions:\", outputs.shape)"
      ],
      "metadata": {
        "id": "W4C-4K4ioEX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca9bdd5-9ead-49f9-846a-09e6fd0b2f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs: tensor([[[-1.5854,  0.9904],\n",
            "         [-3.7235,  7.4548],\n",
            "         [-2.2661,  6.6049],\n",
            "         [-3.5983,  3.9902]]])\n",
            "Outputs dimensions: torch.Size([1, 4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Last output token:\", outputs[:, -1, :])"
      ],
      "metadata": {
        "id": "KUU5goVRoMoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5d1a2c-b0a9-453b-da93-65d79394d42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output token: tensor([[-3.5983,  3.9902]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  6.6 Calculating the classification loss and accuracy"
      ],
      "metadata": {
        "id": "zAiHnCawoRT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Last output token:\", outputs[:, -1, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RKQw6mgoQuJ",
        "outputId": "8cea4b53-aba0-49d4-f8dd-44287039ef91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output token: tensor([[-3.5983,  3.9902]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取最后一个时间步的输出，并应用 softmax 函数\n",
        "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
        "\n",
        "# 预测类别标签\n",
        "label = torch.argmax(probas).item()\n",
        "\n",
        "# 打印类别标签\n",
        "print(\"Class label:\", label)"
      ],
      "metadata": {
        "id": "tD1QhrcErS3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4dc474-dee4-44d4-e4dc-5096ae4e57b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取最后一个时间步的 logits\n",
        "logits = outputs[:, -1, :]\n",
        "\n",
        "# 预测类别标签\n",
        "label = torch.argmax(logits)\n",
        "\n",
        "# 打印类别标签\n",
        "print(\"Class label:\", label.item())"
      ],
      "metadata": {
        "id": "mGsrDlyfrZig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855fd5be-66e2-4b29-80ea-0be5da05a120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    # 设置模型为评估模式\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    # 确定要处理的批次数量\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    # 遍历数据加载器\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            # 将输入和目标张量移动到指定设备\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            # 在不计算梯度的上下文中进行推理\n",
        "            with torch.no_grad():\n",
        "                # 获取模型输出\n",
        "                logits = model(input_batch)[:, -1, :]\n",
        "                # 预测类别标签\n",
        "                predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # 更新统计信息\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # 返回准确率\n",
        "    return correct_predictions / num_examples"
      ],
      "metadata": {
        "id": "Ds-Vqltyrd7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 确定设备（GPU 或 CPU）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 将模型移动到指定设备\n",
        "model.to(device)\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 计算训练集准确率\n",
        "train_accuracy = calc_accuracy_loader(\n",
        "    train_loader, model, device, num_batches=10\n",
        ")\n",
        "\n",
        "# 计算验证集准确率\n",
        "val_accuracy = calc_accuracy_loader(\n",
        "    val_loader, model, device, num_batches=10\n",
        ")\n",
        "\n",
        "# 计算测试集准确率\n",
        "test_accuracy = calc_accuracy_loader(\n",
        "    test_loader, model, device, num_batches=10\n",
        ")\n",
        "\n",
        "# 打印准确率\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "skh47G7grkr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72d414f-5262-4ebb-bfcf-ff26c5015584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 46.25%\n",
            "Validation accuracy: 45.00%\n",
            "Test accuracy: 48.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    # 将输入和目标张量移动到指定设备\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    # 获取模型输出\n",
        "    logits = model(input_batch)[:, -1, :]\n",
        "\n",
        "    # 计算交叉熵损失\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "\n",
        "    # 返回损失\n",
        "    return loss"
      ],
      "metadata": {
        "id": "dqtaV5ICrqbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    # 初始化总损失\n",
        "    total_loss = 0\n",
        "\n",
        "    # 处理空数据加载器\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    # 确定要处理的批次数量\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    # 遍历数据加载器\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            # 计算一个批次的损失\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "            # 累加损失\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # 返回平均损失\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "6206QxBWrxDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 在不计算梯度的上下文中计算损失\n",
        "with torch.no_grad():\n",
        "    # 计算训练集损失\n",
        "    train_loss = calc_loss_loader(\n",
        "        train_loader, model, device, num_batches=5\n",
        "    )\n",
        "\n",
        "    # 计算验证集损失\n",
        "    val_loss = calc_loss_loader(\n",
        "        val_loader, model, device, num_batches=5\n",
        "    )\n",
        "\n",
        "    # 计算测试集损失\n",
        "    test_loss = calc_loss_loader(\n",
        "        test_loader, model, device, num_batches=5\n",
        "    )\n",
        "\n",
        "# 打印损失\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "id": "eS1nFKo1r5ex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05679bd-bc17-48d0-9d6e-e8f8dd8d2e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.453\n",
            "Validation loss: 2.583\n",
            "Test loss: 2.322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  6.7 Fine-tuning the model on supervised data"
      ],
      "metadata": {
        "id": "_ClAxqDRr_5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs, eval_freq, eval_iter\n",
        "):\n",
        "    # 初始化统计信息列表\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # 主训练循环\n",
        "    for epoch in range(num_epochs):\n",
        "        # 设置模型为训练模式\n",
        "        model.train()\n",
        "\n",
        "        # 遍历训练集\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            # 重置梯度\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 计算损失\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "\n",
        "            # 反向传播\n",
        "            loss.backward()\n",
        "\n",
        "            # 更新参数\n",
        "            optimizer.step()\n",
        "\n",
        "            # 更新统计信息\n",
        "            examples_seen += input_batch.shape[0]\n",
        "            global_step += 1\n",
        "\n",
        "            # 评估模型\n",
        "            if global_step % eval_freq == 0:\n",
        "                # 计算训练集和验证集损失\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter\n",
        "                )\n",
        "\n",
        "                # 记录损失\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "\n",
        "                # 打印损失\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, \"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # 计算准确率\n",
        "        train_accuracy = calc_accuracy_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_accuracy = calc_accuracy_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "        # 打印准确率\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "\n",
        "        # 记录准确率\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    # 返回统计信息\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ],
      "metadata": {
        "id": "YJ2asc1ur-7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    # 设置模型为评估模式\n",
        "    model.eval()\n",
        "\n",
        "    # 在不计算梯度的上下文中进行评估\n",
        "    with torch.no_grad():\n",
        "        # 计算训练集损失\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "        # 计算验证集损失\n",
        "        val_loss = calc_loss_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "    # 设置模型为训练模式\n",
        "    model.train()\n",
        "\n",
        "    # 返回损失\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "-mliG3DAsJcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# 记录开始时间\n",
        "start_time = time.time()\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 初始化优化器\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "# 训练轮数\n",
        "num_epochs = 5\n",
        "\n",
        "# 训练模型\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
        "    train_classifier_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=50,\n",
        "        eval_iter=5\n",
        "    )\n",
        "\n",
        "# 记录结束时间\n",
        "end_time = time.time()\n",
        "\n",
        "# 计算训练时间（分钟）\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "\n",
        "# 打印训练时间\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "2ZtpfFzesRDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e48c3e-ff90-4d75-d593-4eaed2a7e4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
            "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
            "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
            "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
            "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
            "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
            "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
            "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
            "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
            "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
            "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
            "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
            "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
            "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
            "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Training completed in 49.97 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入绘图库\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# 定义绘图函数\n",
        "def plot_values(\n",
        "    epochs_seen, examples_seen, train_values, val_values,\n",
        "    label=\"loss\"\n",
        "):\n",
        "    # 创建绘图窗口和子图\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # 绘制训练和验证曲线\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(\n",
        "        epochs_seen, val_values, linestyle=\"--\",\n",
        "        label=f\"Validation {label}\"\n",
        "    )\n",
        "\n",
        "    # 设置坐标轴标签和图例\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # 创建第二个x轴\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    # 调整布局并保存显示图表\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "# 生成数据并调用绘图函数\n",
        "# 假设 num_epochs 和 examples_seen 已定义，train_losses 和 val_losses 包含损失值数据\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "O0HIFFDIsYcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成准确率的 epochs 和 examples seen 张量\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "\n",
        "# 绘制准确率曲线\n",
        "plot_values(\n",
        "    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n",
        "    label=\"accuracy\"\n",
        ")"
      ],
      "metadata": {
        "id": "2rDYb-3gs5JE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "ef315e6e-5fac-4abf-a3b2-2f470090090f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算训练集准确率\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "\n",
        "# 计算验证集准确率\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "\n",
        "# 计算测试集准确率\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "# 打印准确率\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "sb_qeB2-tAIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a1002d-f0e8-4b02-94a8-b2e19ba18a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 97.21%\n",
            "Validation accuracy: 97.32%\n",
            "Test accuracy: 95.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.8 Using the LLM as a spam classifier"
      ],
      "metadata": {
        "id": "zngBXtrutGfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(\n",
        "    text, model, tokenizer, device, max_length=None,\n",
        "    pad_token_id=50256\n",
        "):\n",
        "    # 设置模型为评估模式\n",
        "    model.eval()\n",
        "\n",
        "    # 编码输入文本\n",
        "    input_ids = tokenizer.encode(text)\n",
        "\n",
        "    # 获取模型支持的上下文长度\n",
        "    supported_context_length = model.pos_emb.weight.shape[1]\n",
        "\n",
        "    # 截断输入序列（如果过长）\n",
        "    if max_length is not None:\n",
        "        input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "    else:\n",
        "        input_ids = input_ids[:supported_context_length]\n",
        "\n",
        "    # 填充输入序列\n",
        "    if max_length is not None:\n",
        "        input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "\n",
        "    # 转换为张量并添加批次维度\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "\n",
        "    # 在不计算梯度的上下文中进行推理\n",
        "    with torch.no_grad():\n",
        "        # 获取模型输出\n",
        "        logits = model(input_tensor)[:, -1, :]\n",
        "        # 预测类别标签\n",
        "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # 返回分类结果\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ],
      "metadata": {
        "id": "kf3WSk81tGBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义输入文本\n",
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "# 对输入文本进行分类并打印结果\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ],
      "metadata": {
        "id": "zbT1yY7FtQKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9d93f9-e428-4e08-b8ba-c9621258728c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义输入文本\n",
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "# 对输入文本进行分类并打印结果\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ],
      "metadata": {
        "id": "_QAmLee5tVF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9f1df3-83a0-4f34-89ec-e2db386fa013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), \"review_classifier.pth\")"
      ],
      "metadata": {
        "id": "gllwLV2NtaSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载模型的状态字典\n",
        "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
        "\n",
        "# 将状态字典加载到模型中\n",
        "model.load_state_dict(model_state_dict)"
      ],
      "metadata": {
        "id": "xYgHYLEhthGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Fine-tuning to follow instructions"
      ],
      "metadata": {
        "id": "h3Oa-qTDvDnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1  Introduction to instruction fine-tuning"
      ],
      "metadata": {
        "id": "Y9lfR0sevHpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Preparing a dataset for supervised instruction fine-tuning"
      ],
      "metadata": {
        "id": "uCloov8nvLAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    # 检查文件是否已存在\n",
        "    if not os.path.exists(file_path):\n",
        "        # 下载文件\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        # 保存文件\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        # 读取本地文件\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    # 加载 JSON 数据\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 定义文件路径和 URL\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "# 下载并加载文件\n",
        "data = download_and_load_file(file_path, url)\n",
        "\n",
        "# 打印数据条目数\n",
        "print(\"Number of entries:\", len(data))"
      ],
      "metadata": {
        "id": "9IG7kRd3vGHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印示例条目\n",
        "print(\"Example entry:\\n\", data[50])"
      ],
      "metadata": {
        "id": "sLxadhoHbrHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Another example entry:\\n\", data[999])"
      ],
      "metadata": {
        "id": "60KkWMtWb11q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "    # 构建指令文本\n",
        "    instruction_text = (\n",
        "        f'\"Write is an instruction that describes a task. \"\\n'\n",
        "        f'\"Below is a response that appropriately completes the request.\"\\n'\n",
        "        f'\\n### Instruction:\\n{entry[\"instruction\"]}'\n",
        "    )\n",
        "\n",
        "    # 构建输入文本\n",
        "    input_text = (\n",
        "        f'\\n### Input:\\n{entry[\"input\"]}' if entry[\"input\"] else \"\"\n",
        "    )\n",
        "\n",
        "    # 返回格式化的输入文本\n",
        "    return instruction_text + input_text"
      ],
      "metadata": {
        "id": "ngeZq93nb4sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 格式化输入文本\n",
        "model_input = format_input(data[50])\n",
        "\n",
        "# 生成期望响应\n",
        "desired_response = f'\\n### Response:\\n{data[50][\"output\"]}'\n",
        "\n",
        "# 打印组合文本\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebkhGoCdcA2c",
        "outputId": "e212f509-f26a-4ca3-8ecc-fc18e8734fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Write is an instruction that describes a task. \"\n",
            "\"Below is a response that appropriately completes the request.\"\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "### Input:\n",
            "Ocassion\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 格式化输入文本\n",
        "model_input = format_input(data[999])\n",
        "\n",
        "# 生成期望响应\n",
        "desired_response = f'\\n### Response:\\n{data[999][\"output\"]}'\n",
        "\n",
        "# 打印组合文本\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0_LfmricGzo",
        "outputId": "af50f10d-e69b-4b2e-d264-cbbc98250ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Write is an instruction that describes a task. \"\n",
            "\"Below is a response that appropriately completes the request.\"\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "### Response:\n",
            "An antonym of 'complicated' is 'simple'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算各数据集的大小\n",
        "train_portion = int(len(data) * 0.85)  # 训练集占比 85%\n",
        "test_portion = int(len(data) * 0.1)    # 测试集占比 10%\n",
        "val_portion = len(data) - train_portion - test_portion  # 验证集占比 5%\n",
        "\n",
        "# 划分数据集\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "# 打印各数据集的长度\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK1479GbcOmy",
        "outputId": "9d77b015-55ea-4a7e-dba0-810d082efa7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Organizing data into training batches"
      ],
      "metadata": {
        "id": "JVIz3Hm1cWxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        # 初始化数据集\n",
        "        self.data = data\n",
        "        self.encoded_texts = []\n",
        "\n",
        "        # 预处理数据\n",
        "        for entry in data:\n",
        "            # 格式化输入文本\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            # 生成响应文本\n",
        "            response_text = f'\\n### Response:\\n{entry[\"output\"]}'\n",
        "            # 组合输入和响应文本\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            # 编码文本\n",
        "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 获取指定索引的编码文本\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # 返回数据集的长度\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "1Y1AAlg_cV50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# 获取 GPT-2 的分词器\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 编码文本并打印结果\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Xzdqn8chUv",
        "outputId": "4f29b0be-2a0f-4917-e691-1d185d121a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_1(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # 计算批次中最长序列的长度\n",
        "    batch_max_length = max(len(item) + 1 for item in batch)\n",
        "\n",
        "    # 初始化输入列表\n",
        "    inputs_list = []\n",
        "\n",
        "    # 处理每个序列\n",
        "    for item in batch:\n",
        "        # 复制并扩展序列\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_token_id]\n",
        "\n",
        "        # 填充序列到最大长度\n",
        "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "\n",
        "        # 转换为张量并移除最后一个填充 token\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        inputs_list.append(inputs)\n",
        "\n",
        "    # 堆叠张量并移动到目标设备\n",
        "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
        "\n",
        "    return inputs_tensor"
      ],
      "metadata": {
        "id": "Trbe2sxIcpOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义输入序列\n",
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "# 构建批次数据\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3,\n",
        ")\n",
        "\n",
        "# 处理批次数据并打印结果\n",
        "print(custom_collate_draft_1(batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxN5iOUlcwPN",
        "outputId": "25eada5a-edf7-4327-ae7c-7c7c6d1f6578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # 计算批次中最长序列的长度\n",
        "    batch_max_length = max(len(item) + 1 for item in batch)\n",
        "\n",
        "    # 初始化输入和目标列表\n",
        "    inputs_list, targets_list = [], []\n",
        "\n",
        "    # 处理每个序列\n",
        "    for item in batch:\n",
        "        # 复制并扩展序列\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_token_id]\n",
        "\n",
        "        # 填充序列到最大长度\n",
        "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "\n",
        "        # 生成输入和目标张量\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        targets = torch.tensor(padded[1:])\n",
        "\n",
        "        # 添加到列表\n",
        "        inputs_list.append(inputs)\n",
        "        targets_list.append(targets)\n",
        "\n",
        "    # 堆叠张量并移动到目标设备\n",
        "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
        "    targets_tensor = torch.stack(targets_list).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor\n",
        "\n",
        "# 定义输入序列\n",
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "# 构建批次数据\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3,\n",
        ")\n",
        "\n",
        "# 处理批次数据并打印结果\n",
        "inputs, targets = custom_collate_draft_2(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMgn2rRdc3Gz",
        "outputId": "f2d9e4b7-7abf-46a7-8282-fde567d10a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256, 50256, 50256, 50256],\n",
            "        [    8,     9, 50256, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # 计算批次中最长序列的长度\n",
        "    batch_max_length = max(len(item) + 1 for item in batch)\n",
        "\n",
        "    # 初始化输入和目标列表\n",
        "    inputs_list, targets_list = [], []\n",
        "\n",
        "    # 处理每个序列\n",
        "    for item in batch:\n",
        "        # 复制并扩展序列\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_token_id]\n",
        "\n",
        "        # 填充序列到最大长度\n",
        "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "\n",
        "        # 生成输入和目标张量\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        targets = torch.tensor(padded[1:])\n",
        "\n",
        "        # 处理掩码\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # 截断到最大长度（如果指定）\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        # 添加到列表\n",
        "        inputs_list.append(inputs)\n",
        "        targets_list.append(targets)\n",
        "\n",
        "    # 堆叠张量并移动到目标设备\n",
        "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
        "    targets_tensor = torch.stack(targets_list).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "y78wkGOxc_9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 处理批次数据并打印结果\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuqI8v48dLT3",
        "outputId": "2587a6e8-d340-4867-d08c-fec14caf4df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 定义预测结果和目标\n",
        "logits_1 = torch.tensor([\n",
        "    [-1.0, 1.0],  # 第一个 token 的预测结果\n",
        "    [-0.5, 1.5]   # 第二个 token 的预测结果\n",
        "])\n",
        "targets_1 = torch.tensor([0, 1])  # 正确的 token 索引\n",
        "\n",
        "# 计算交叉熵损失\n",
        "loss_1 = F.cross_entropy(logits_1, targets_1)\n",
        "\n",
        "# 打印损失\n",
        "print(loss_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3gHpFANdRJ7",
        "outputId": "05e05649-9530-45df-c2c3-eb85cc95f2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 定义预测结果和目标\n",
        "logits_2 = torch.tensor([\n",
        "    [-1.0, 1.0],  # 第一个 token 的预测结果\n",
        "    [-0.5, 1.5],  # 第二个 token 的预测结果\n",
        "    [-0.5, 1.5]   # 第三个 token 的预测结果\n",
        "])\n",
        "targets_2 = torch.tensor([0, 1, 1])  # 正确的 token 索引\n",
        "\n",
        "# 计算交叉熵损失\n",
        "loss_2 = F.cross_entropy(logits_2, targets_2)\n",
        "\n",
        "# 打印损失\n",
        "print(loss_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX6Was11dX3M",
        "outputId": "2ac16123-a6c6-4aac-c7fc-28e70c3276d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7936)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 定义预测结果和目标\n",
        "logits_2 = torch.tensor([\n",
        "    [-1.0, 1.0],  # 第一个 token 的预测结果\n",
        "    [-0.5, 1.5],  # 第二个 token 的预测结果\n",
        "    [-0.5, 1.5]   # 第三个 token 的预测结果\n",
        "])\n",
        "targets_3 = torch.tensor([0, 1, -100])  # 正确的 token 索引，其中 -100 表示忽略\n",
        "\n",
        "# 计算交叉熵损失\n",
        "loss_3 = F.cross_entropy(logits_2, targets_3)\n",
        "\n",
        "# 打印损失\n",
        "print(loss_3)\n",
        "\n",
        "# 比较损失值是否相等\n",
        "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy5vANZ0dgXt",
        "outputId": "7747c272-4fe9-4d55-ca77-4f96ccc48b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.1269)\n",
            "loss_1 == loss_3: tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Creating data loaders for an instruction dataset"
      ],
      "metadata": {
        "id": "JG-j0z8_drmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 选择计算设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 对于 Apple Silicon 芯片，取消以下两行的注释以使用 GPU\n",
        "# if torch.backends.mps.is_available():\n",
        "#     device = torch.device(\"mps\")\n",
        "\n",
        "# 打印设备信息\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G9l3isadppJ",
        "outputId": "5c17614f-003c-4d5d-f082-07e4af84e165"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "# 定制 collate 函数\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "sod6Oabdd1pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 定义数据加载器参数\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "# 创建训练集数据加载器\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "# 创建验证集数据加载器\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "# 创建测试集数据加载器\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "_Rn_kODReDbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印训练集数据加载器信息\n",
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdh1NgN6eLHy",
        "outputId": "4ac67fba-e1e0-4d5d-e42f-089a9a3e9f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 Loading a pretrained LLM"
      ],
      "metadata": {
        "id": "gQ7vM5BYeUiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "# from chapter05 import load_weights_into_gpt\n",
        "\n",
        "# 基础配置\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,    # 词汇表大小\n",
        "    \"context_length\": 1024, # 上下文长度\n",
        "    \"drop_rate\": 0.0,       # 丢弃率\n",
        "    \"qkv_bias\": True        # 查询-键-值偏置\n",
        "}\n",
        "\n",
        "# 模型配置\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# 选择模型\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "# 获取模型大小\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "\n",
        "# 下载并加载模型权重\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "# 初始化模型并加载权重\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKqF_-jBeRD2",
        "outputId": "f5c8374e-6284-46df-a4eb-4c14f51978ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 71.2kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.19MiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 51.4kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [01:14<00:00, 19.1MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 9.64MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 2.85MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.04MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 1024)\n",
              "  (pos_emb): Embedding(1024, 1024)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 格式化验证集中的一个条目\n",
        "input_text = format_input(val_data[0])\n",
        "\n",
        "# 打印格式化后的输入文本\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxLyWb-fegcn",
        "outputId": "06633a12-2448-4f01-cd52-74ff23ced6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Write is an instruction that describes a task. \"\n",
            "\"Below is a response that appropriately completes the request.\"\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from chapter05 import generate, text_to_token_ids, token_ids_to_text\n",
        "\n",
        "# 生成文本\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "\n",
        "# 将 token ID 转换为文本\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ],
      "metadata": {
        "id": "0ZcUjAXqepAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取响应文本\n",
        "response_text = generated_text[len(input_text):].strip()\n",
        "\n",
        "# 打印响应文本\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvLvUONUeyPI",
        "outputId": "700543e5-588f-4099-edde-ba7ee115d9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Response:\n",
            "\n",
            "Write is an instruction that describes a task. \"\n",
            "\n",
            "\"Below is a response that appropriately completes the request.\"\n",
            "\n",
            "### Instruction:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.6 Fine-tuning the LLM on instruction data"
      ],
      "metadata": {
        "id": "sxiUYvLLe_Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from chapter05 import (\n",
        "#     calc_loss_loader,\n",
        "#     train_model_simple\n",
        "# )"
      ],
      "metadata": {
        "id": "2Xyecj1qfCGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将模型移动到目标设备\n",
        "model.to(device)\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 计算训练集损失\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(\n",
        "        train_loader, model, device, num_batches=5\n",
        "    )\n",
        "\n",
        "# 计算验证集损失\n",
        "    val_loss = calc_loss_loader(\n",
        "        val_loader, model, device, num_batches=5\n",
        "    )\n",
        "\n",
        "# 打印损失值\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0EtOKVI1fJrq",
        "outputId": "c1def906-476c-4830-c258-30a59062d1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "0D or 1D target tensor expected, multi-target not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3320564533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 计算训练集损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     train_loss = calc_loss_loader(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-3141209801.py\u001b[0m in \u001b[0;36mcalc_loss_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# 计算一个批次的损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             loss = calc_loss_batch(\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             )\n",
            "\u001b[0;32m/tmp/ipython-input-3695053306.py\u001b[0m in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 计算交叉熵损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 返回损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# 记录开始时间\n",
        "start_time = time.time()\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 初始化优化器\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
        ")\n",
        "\n",
        "# 定义训练轮数\n",
        "num_epochs = 2\n",
        "\n",
        "# 训练模型\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# 记录结束时间\n",
        "end_time = time.time()\n",
        "\n",
        "# 计算执行时间（分钟）\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "\n",
        "# 打印训练完成信息\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "1xduuA7XfQkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from chapter05 import plot_losses\n",
        "\n",
        "# 创建 epoch 张量\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "\n",
        "# 绘制损失图表\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "WGy_BvR6fXvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.7 Extracting and saving responses"
      ],
      "metadata": {
        "id": "klHjFispfgKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 遍历测试集的前三个样本\n",
        "for entry in test_data[:3]:\n",
        "    # 格式化输入文本\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    # 生成文本\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "\n",
        "    # 将 token ID 转换为文本\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    # 提取响应文本\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    # 打印输入文本、正确响应和模型响应\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n> {entry['output']}\")\n",
        "    print(f\"Model response:\\n> {response_text.strip()}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "vFHk_t-IfiaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 遍历测试集的所有样本\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "    # 格式化输入文本\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    # 生成文本\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "\n",
        "    # 将 token ID 转换为文本\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    # 提取响应文本\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    # 保存响应文本到测试数据中\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "# 将测试数据保存到 JSON 文件\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)"
      ],
      "metadata": {
        "id": "dJK6iLTMfteY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[0])"
      ],
      "metadata": {
        "id": "xjUUWXqAf2hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 生成文件名\n",
        "file_name = f\"{re.sub(r'[()]', '', CHOOSE_MODEL)}-sft.pth\"\n",
        "\n",
        "# 保存模型状态字典\n",
        "torch.save(model.state_dict(), file_name)\n",
        "\n",
        "# 打印保存信息\n",
        "print(f\"Model saved as {file_name}\")"
      ],
      "metadata": {
        "id": "TyquXTAZf30k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.8 Evaluating the fine-tuned LLM"
      ],
      "metadata": {
        "id": "A5TK5EtlgAsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama serve"
      ],
      "metadata": {
        "id": "NpwQcCMbgDQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama run llama3"
      ],
      "metadata": {
        "id": "0LPtnXpfgGjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "# 定义函数检查进程是否运行\n",
        "def check_if_running(process_name):\n",
        "    running = False\n",
        "    for proc in psutil.process_iter([\"name\"]):\n",
        "        if process_name in proc.info[\"name\"]:\n",
        "            running = True\n",
        "            break\n",
        "    return running\n",
        "\n",
        "# 检查 Ollama 是否运行\n",
        "ollama_running = check_if_running(\"ollama\")\n",
        "\n",
        "# 如果 Ollama 未运行，抛出异常\n",
        "if not ollama_running:\n",
        "    raise RuntimeError(\n",
        "        \"Ollama not running. Launch ollama before proceeding.\"\n",
        "    )\n",
        "\n",
        "# 打印 Ollama 运行状态\n",
        "print(\"Ollama running:\", check_if_running(\"ollama\"))"
      ],
      "metadata": {
        "id": "erT7xuZXgmph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import urllib.request\n",
        "\n",
        "# 定义函数查询模型\n",
        "def query_model(prompt, model=\"llama3\", url=\"http://localhost:11434/api/chat\"):\n",
        "    # 创建请求数据\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"options\": {\n",
        "            \"seed\": 123,\n",
        "            \"temperature\": 0,\n",
        "            \"num_ctx\": 2048\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # 将数据转换为 JSON 格式并编码\n",
        "    payload = json.dumps(data).encode(\"utf-8\")\n",
        "\n",
        "    # 创建请求对象\n",
        "    request = urllib.request.Request(\n",
        "        url,\n",
        "        data=payload,\n",
        "        method=\"POST\"\n",
        "    )\n",
        "\n",
        "    # 添加请求头\n",
        "    request.add_header(\"Content-Type\", \"application/json\")\n",
        "\n",
        "    # 发送请求并获取响应\n",
        "    response_data = \"\"\n",
        "    with urllib.request.urlopen(request) as response:\n",
        "        while True:\n",
        "            line = response.readline().decode(\"utf-8\")\n",
        "            if not line:\n",
        "                break\n",
        "            response_json = json.loads(line)\n",
        "            response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    # 返回响应数据\n",
        "    return response_data"
      ],
      "metadata": {
        "id": "cJ1mox8fgnOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型名称\n",
        "model = \"llama3\"\n",
        "\n",
        "# 查询模型\n",
        "result = query_model(\"What do Llamas eat?\", model)\n",
        "\n",
        "# 打印响应\n",
        "print(result)"
      ],
      "metadata": {
        "id": "nzho7YXxgzXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 遍历测试数据的前三个样本\n",
        "for entry in test_data[:3]:\n",
        "    # 生成评分提示\n",
        "    prompt = (\n",
        "        f\"Given the input `{format_input(entry)}` \"\n",
        "        f\"and correct output `{entry['output']}`, \"\n",
        "        f\"score the model response `{entry['model_response']}` \"\n",
        "        f\"on a scale from 0 to 100, where 100 is the best score. \"\n",
        "    )\n",
        "\n",
        "    # 打印数据集响应、模型响应和评分提示\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">\", query_model(prompt))\n",
        "    print(\"\\n\" + \"-\" * 50)"
      ],
      "metadata": {
        "id": "UwyGpiySg59N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# 定义函数生成模型评分\n",
        "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        # 生成评分提示\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}` \"\n",
        "            f\"on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "\n",
        "        # 查询模型获取评分\n",
        "        score = query_model(prompt, model)\n",
        "\n",
        "        # 尝试将评分转换为整数并添加到列表\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "    return scores"
      ],
      "metadata": {
        "id": "vDPSTO_ShC2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成模型评分\n",
        "scores = generate_model_scores(test_data, \"model_response\")\n",
        "\n",
        "# 打印评分的数量\n",
        "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
        "\n",
        "# 打印平均分\n",
        "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
      ],
      "metadata": {
        "id": "tlhPWMxBhNJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}